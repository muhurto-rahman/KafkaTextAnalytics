{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04e6768f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing 5200 METAMORPHOSIS\n",
      "Importing  /Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/KafkaFinal/data\\pg5200.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone ^\\s*[IVXLCM]+\\s*$\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n",
      "Tokenizing 5252 BLUMFELD\n",
      "Importing  /Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/KafkaFinal/data\\pg5252.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone NOCHAPTERS\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n",
      "Tokenizing 5353 THE BURROW\n",
      "Importing  /Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/KafkaFinal/data\\pg5353.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone NOCHAPTERS\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n",
      "Tokenizing 5454 JOSEPHINE THE SONGSTRESS\n",
      "Importing  /Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/KafkaFinal/data\\pg5454.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone NOCHAPTERS\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n",
      "Tokenizing 5555 BEFORE THE LAW\n",
      "Importing  /Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/KafkaFinal/data\\pg5555.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone NOCHAPTERS\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n",
      "Tokenizing 5656 UP IN THE GALLERY\n",
      "Importing  /Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/KafkaFinal/data\\pg5656.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone NOCHAPTERS\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n",
      "Tokenizing 5757 THE HUNTER GRACCHUS\n",
      "Importing  /Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/KafkaFinal/data\\pg5757.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone NOCHAPTERS\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n",
      "Tokenizing 5858 THE GREAT WALL OF CHINA\n",
      "Importing  /Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/KafkaFinal/data\\pg5858.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone NOCHAPTERS\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n",
      "Tokenizing 5959 A REPORT FOR AN ACADEMY\n",
      "Importing  /Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/KafkaFinal/data\\pg5959.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone NOCHAPTERS\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n",
      "Tokenizing 6060 DEAREST FATHER\n",
      "Importing  /Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/KafkaFinal/data\\pg6060.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone NOCHAPTERS\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n",
      "Tokenizing 6161 THE JUDGEMENT\n",
      "Importing  /Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/KafkaFinal/data\\pg6161.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone NOCHAPTERS\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n",
      "Tokenizing 6262 AMERIKA\n",
      "Importing  /Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/KafkaFinal/data\\pg6262.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone ^\\s*\\d+\\s*$\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n",
      "Tokenizing 6363 IN THE PENAL COLONY\n",
      "Importing  /Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/KafkaFinal/data\\pg6363.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone NOCHAPTERS\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n",
      "Tokenizing 6464 THE HUNGER ARTIST\n",
      "Importing  /Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/KafkaFinal/data\\pg6464.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone NOCHAPTERS\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n",
      "Tokenizing 6565 THE JACKALS AND ARABS\n",
      "Importing  /Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/KafkaFinal/data\\pg6565.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone NOCHAPTERS\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n",
      "Tokenizing 6666 A COUNTRY DOCTOR\n",
      "Importing  /Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/KafkaFinal/data\\pg6666.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone NOCHAPTERS\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n",
      "Tokenizing 6767 AN IMPERIAL MESSAGE\n",
      "Importing  /Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/KafkaFinal/data\\pg6767.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone NOCHAPTERS\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n",
      "Tokenizing 6969 THE CASTLE\n",
      "Importing  /Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/KafkaFinal/data\\pg6969.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone ^\\s*\\d+\\s*$\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n",
      "Tokenizing 7849 THE TRIAL\n",
      "Importing  /Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/KafkaFinal/data\\pg7849.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone ^\\s*Chapter\\s+(?:One|Two|Three|Four|Five|Six|Seven|Eight|Nine|Ten)\\s*$\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n",
      "Tokenizing 23532 MEDITATION\n",
      "Importing  /Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/KafkaFinal/data\\pg23532.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone ^(Children on the country road|Unmasking a con artist|The Sudden Walk|Resolutions|The trip to the mountains|The Bachelor's Misfortune|The Merchant|Distracted Looking Out|The Way Home|The Passers-by|Passenger|Dresses|The rejection|Food for thought for gentlemen riders|The Alley Window|Desire to become an Indian|The Trees|Unhappiness)$\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/DS5001-2025-01-R/lessons/lib\\textparser.py:132: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  div_lines = self.TOKENS[src_col].str.contains(div_pat, regex=True, case=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import re\n",
    "import nltk\n",
    "import plotly_express as px\n",
    "import configparser\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import re\n",
    "import nltk\n",
    "import plotly_express as px\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "import scipy.cluster.hierarchy as sch\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.linalg import norm\n",
    "from scipy.spatial.distance import pdist\n",
    "import plotly_express as px\n",
    "import seaborn as sns; sns.set()\n",
    "import configparser\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\"../../../env-sample.ini\")\n",
    "data_home = '/Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/KafkaFinal/data'\n",
    "output_dir = '/Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/KafkaFinal/output'\n",
    "local_lib = '/Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/DS5001-2025-01-R/lessons/lib'\n",
    "import sys\n",
    "sys.path.append(local_lib)\n",
    "from textparser import TextParser\n",
    "\n",
    "clip_pats = [\n",
    "    r\"\\*\\*\\*\\s*START OF\",\n",
    "    r\"\\*\\*\\*\\s*END OF\"\n",
    "]\n",
    "\n",
    "# All are 'chap'and 'm'\n",
    "roman = '[IVXLCM]+'\n",
    "caps = \"[A-Z';, -]+\"\n",
    "ohco_pat_list = [\n",
    "    (5200,   rf\"^\\s*CHAPTER\\s+{roman}\\s*$\"), #Metamorphosis\n",
    "    (7849,   rf\"^\\s*{roman}\\s*$\"), #The Trial\n",
    "    (6969,  rf\"^\\s*LETTER .* to .*$\"), # The Castle\n",
    "    (6262,   rf\"^CHAPTER\\s+{roman}$\"), # Amerika\n",
    "    (6161,   rf\"^CHAPTER\\s+\\d+$\"), # The Judgement\n",
    "    (6060,   rf\"^Chapter\\s+\\d+$\"), # Dearest Father\n",
    "    (6363,  rf\"^Chapter\\s+\\d+$\"), # In the Penal colony\n",
    "    (6464,   rf\"^CHAPTER\\s+\\d+$\"), # The Hunger Artist\n",
    "    (6565, rf\"^\\s*CHAPTER\\s+{roman}\\.\"), # The Jackals and Arabs\n",
    "    (6666, rf\"^\\s*CHAPTER\\s+{roman}\\s*$\"), # A Country Doctor\n",
    "    (6767, rf\"^\\s*CHAPTER\\s+{roman}\\s*$\"), # An Imperial Message\n",
    "    (5959,  rf\"^(?:ETYMOLOGY|EXTRACTS|CHAPTER)\"), # A report for an Academy\n",
    "    (5858,  rf\"^\\s*CHAPTER\\s+{roman}\\.\\s*$\"), # The Great Wall of China\n",
    "    (5757, rf\"^\\s*{roman}\\.\\s*$\"), # The Hunter Gracchus\n",
    "    (5656,  rf\"^\\s*{roman}\\. .*$\"), # Up in the Gallery\n",
    "    (5555, rf\"^CHAPTER\\s+{roman}\\.?$\"), # Before the Law\n",
    "    (5454, rf\"^\\s*[A-Z,;-]+\\.\\s*$\"), # Josephine the Songstress\n",
    "    (5353,  rf\"^CHAPTER \"), # The Burrow\n",
    "    (5252, rf\"^CHAPTER\\s+{roman}\\.\\s*$\"), # Blumfeld\n",
    "    (23532, rf\"Chapter\\s+{roman}\") # Meditation\n",
    "]\n",
    "chapter_regexes = [\n",
    "    (5200,   rf\"^\\s*{roman}\\s*$\"),\n",
    "    (7849,   rf\"^\\s*Chapter\\s+(?:One|Two|Three|Four|Five|Six|Seven|Eight|Nine|Ten)\\s*$\"),\n",
    "    (6969,   rf\"^\\s*\\d+\\s*$\"),\n",
    "    (6262,   rf\"^\\s*\\d+\\s*$\"),\n",
    "    (6161,   \"NOCHAPTERS\"),\n",
    "    (6060,   \"NOCHAPTERS\"),\n",
    "    (6363,   \"NOCHAPTERS\"),\n",
    "    (6464,   \"NOCHAPTERS\"),\n",
    "    (6565,   \"NOCHAPTERS\"),\n",
    "    (6666,   \"NOCHAPTERS\"),\n",
    "    (6767,   \"NOCHAPTERS\"),\n",
    "    (5959,   \"NOCHAPTERS\"),\n",
    "    (5858,   \"NOCHAPTERS\"),\n",
    "    (5757,   \"NOCHAPTERS\"),\n",
    "    (5656,   \"NOCHAPTERS\"),\n",
    "    (5555,   \"NOCHAPTERS\"),\n",
    "    (5454,   \"NOCHAPTERS\"),\n",
    "    (5353,   \"NOCHAPTERS\"),\n",
    "    (5252,   \"NOCHAPTERS\"),\n",
    "    (23532,  rf\"^(Children on the country road|Unmasking a con artist|The Sudden Walk|Resolutions|The trip to the mountains|The Bachelor's Misfortune|The Merchant|Distracted Looking Out|The Way Home|The Passers-by|Passenger|Dresses|The rejection|Food for thought for gentlemen riders|The Alley Window|Desire to become an Indian|The Trees|Unhappiness)$\")  # Poem title on line 1\n",
    "]\n",
    "ohco_pat_list = chapter_regexes\n",
    "source_files = f'{data_home}'\n",
    "source_file_list = sorted(glob(f\"{source_files}/*.*\"))\n",
    "\n",
    "book_data = []\n",
    "for source_file_path in source_file_list:\n",
    "    # Get the filename only, e.g. 'pg5353.txt'\n",
    "    filename = os.path.basename(source_file_path)\n",
    "    # Extract the numeric ID from the filename (remove 'pg' and '.txt')\n",
    "    book_id = int(filename.replace('pg', '').replace('.txt', ''))\n",
    "    # Use filename (without extension) as a raw title (optional: clean further)\n",
    "    book_title = filename.replace('.txt', '').replace('_', ' ')\n",
    "    # Append a tuple of (book_id, path, title)\n",
    "    book_data.append((book_id, source_file_path, book_title))\n",
    "# Convert to DataFrame\n",
    "LIB = pd.DataFrame(book_data, columns=['book_id', 'source_file_path', 'raw_title']) \\\n",
    "        .set_index('book_id') \\\n",
    "        .sort_index()\n",
    "book_titles = {\n",
    "    5200: \"Metamorphosis\",\n",
    "    7849: \"The Trial\",\n",
    "    6969: \"The Castle\",\n",
    "    6262: \"Amerika\",\n",
    "    6161: \"The Judgement\",\n",
    "    6060: \"Dearest Father\",\n",
    "    6363: \"In the Penal Colony\",\n",
    "    6464: \"The Hunger Artist\",\n",
    "    6565: \"The Jackals and Arabs\",\n",
    "    6666: \"A Country Doctor\",\n",
    "    6767: \"An Imperial Message\",\n",
    "    5959: \"A Report for an Academy\",\n",
    "    5858: \"The Great Wall of China\",\n",
    "    5757: \"The Hunter Gracchus\",\n",
    "    5656: \"Up in the Gallery\",\n",
    "    5555: \"Before the Law\",\n",
    "    5454: \"Josephine the Songstress\",\n",
    "    5353: \"The Burrow\",\n",
    "    5252: \"Blumfeld\",\n",
    "    23532: \"Meditation\"\n",
    "}\n",
    "book_titles = {f'pg{key}': value for key, value in book_titles.items()}\n",
    "try:\n",
    "    LIB['author'] = 'KAFKA, FRANZ'\n",
    "    LIB['title'] = LIB.raw_title.replace(book_titles).str.upper()\n",
    "    LIB = LIB.drop('raw_title', axis=1)\n",
    "except AttributeError:\n",
    "    pass\n",
    "LIB['chap_regex'] = LIB.index.map(pd.Series({x[0]:x[1] for x in ohco_pat_list}))\n",
    "LIB\n",
    "# This cell takes 16 seconds to run\n",
    "def tokenize_collection(LIB):\n",
    "\n",
    "    clip_pats = [\n",
    "        r\"\\*\\*\\*\\s*START OF\",\n",
    "        r\"\\*\\*\\*\\s*END OF\"\n",
    "    ]\n",
    "\n",
    "    books = []\n",
    "    for book_id in LIB.index:\n",
    "\n",
    "        # Announce\n",
    "        print(\"Tokenizing\", book_id, LIB.loc[book_id].title)\n",
    "\n",
    "        # Define vars\n",
    "        chap_regex = LIB.loc[book_id].chap_regex\n",
    "        ohco_pats = [('chap', chap_regex, 'm')]\n",
    "        src_file_path = LIB.loc[book_id].source_file_path\n",
    "\n",
    "        # Create object\n",
    "        text = TextParser(src_file_path, ohco_pats=ohco_pats, clip_pats=clip_pats, use_nltk=True)\n",
    "        # text = TextImporter(src_file_path, ohco_pats=ohco_pats, clip_pats=clip_pats) \n",
    "\n",
    "        # Define parameters\n",
    "        text.verbose = True\n",
    "        text.strip_hyphens = True\n",
    "        text.strip_whitespace = True\n",
    "\n",
    "        # Parse\n",
    "        text.import_source().parse_tokens();\n",
    "\n",
    "        # Name things\n",
    "        text.TOKENS['book_id'] = book_id\n",
    "        text.TOKENS = text.TOKENS.reset_index().set_index(['book_id'] + text.OHCO)\n",
    "\n",
    "        # Add to list\n",
    "        books.append(text.TOKENS)\n",
    "        \n",
    "    # Combine into a single dataframe\n",
    "    CORPUS = pd.concat(books).sort_index()\n",
    "\n",
    "    # Clean up\n",
    "    del(books)\n",
    "    del(text)\n",
    "        \n",
    "    print(\"Done\")\n",
    "        \n",
    "    return CORPUS\n",
    "CORPUS = tokenize_collection(LIB)\n",
    "\n",
    "CORPUS = CORPUS[CORPUS.term_str != '']\n",
    "CORPUS['pos_group'] = CORPUS.pos.str[:2]\n",
    "CORPUS\n",
    "VOCAB = CORPUS.term_str.value_counts().to_frame('n').sort_index()\n",
    "VOCAB.index.name = 'term_str'\n",
    "VOCAB['n_chars'] = VOCAB.index.str.len()\n",
    "VOCAB['p'] = VOCAB.n / VOCAB.n.sum()\n",
    "VOCAB['i'] = -np.log2(VOCAB.p)\n",
    "VOCAB['max_pos'] = CORPUS[['term_str','pos']].value_counts().unstack(fill_value=0).idxmax(1)\n",
    "VOCAB['max_pos_group'] = CORPUS[['term_str','pos_group']].value_counts().unstack(fill_value=0).idxmax(1)\n",
    "VOCAB['n_pos_group'] = CORPUS[['term_str','pos_group']].value_counts().unstack().count(1)\n",
    "VOCAB['cat_pos_group'] = CORPUS[['term_str','pos_group']].value_counts().to_frame('n').reset_index()\\\n",
    "    .groupby('term_str').pos_group.apply(lambda x: set(x))\n",
    "VOCAB['n_pos'] = CORPUS[['term_str','pos']].value_counts().unstack().count(1)\n",
    "VOCAB['cat_pos'] = CORPUS[['term_str','pos']].value_counts().to_frame('n').reset_index()\\\n",
    "    .groupby('term_str').pos.apply(lambda x: set(x))\n",
    "sw = pd.DataFrame(nltk.corpus.stopwords.words('english'), columns=['term_str'])\n",
    "sw = sw.reset_index().set_index('term_str')\n",
    "sw.columns = ['dummy']\n",
    "sw.dummy = 1\n",
    "VOCAB['stop'] = VOCAB.index.map(sw.dummy)\n",
    "VOCAB['stop'] = VOCAB['stop'].fillna(0).astype('int')\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer1 = PorterStemmer()\n",
    "VOCAB['porter_stem'] = VOCAB.apply(lambda x: stemmer1.stem(x.name), 1)\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer2 = SnowballStemmer(\"english\")\n",
    "VOCAB['stem_snowball'] = VOCAB.apply(lambda x: stemmer2.stem(x.name), 1)\n",
    "\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "stemmer3 = LancasterStemmer()\n",
    "VOCAB['stem_lancaster'] = VOCAB.apply(lambda x: stemmer3.stem(x.name), 1)\n",
    "\n",
    "VOCAB\n",
    "# Functions to create TFIDF:\n",
    "# Bring into your notebook the functions you created previously to generate a BOW table and compute TFIDF values. \n",
    "# Extend the TFIDF function so that it also returns the DFIDF value for each term in the VOCAB.\n",
    "def gather_docs(CORPUS, ohco_level, term_col='term_str'):\n",
    "    OHCO = CORPUS.index.names\n",
    "    CORPUS[term_col] = CORPUS[term_col].astype('str')\n",
    "    DOC = CORPUS.groupby(OHCO[:ohco_level])[term_col].apply(lambda x:' '.join(x)).to_frame('doc_str')\n",
    "    return DOC\n",
    "\n",
    "def BOW(tokendf, ocholevel):\n",
    "    return tokendf.groupby(bags[ocholevel]+['term_str']).term_str.count().to_frame('n') \n",
    "\n",
    "def BOWtoTFIDF(BOW, tf_method, CORPUS, ocholevel = 2):\n",
    "    \n",
    "    # I added another parameter for the CORPUS because otherwise it becomes really tedious to undo the BOW function\n",
    "\n",
    "    DTCM = BOW.n.unstack(fill_value=0)\n",
    "    if tf_method == 'sum':\n",
    "        TF = DTCM.T / DTCM.T.sum()\n",
    "    elif tf_method == 'max':\n",
    "        TF = DTCM.T / DTCM.T.max()\n",
    "    elif tf_method == 'log':\n",
    "        TF = np.log2(1 + DTCM.T)\n",
    "    elif tf_method == 'raw':\n",
    "        TF = DTCM.T\n",
    "    elif tf_method == 'double_norm':\n",
    "        TF = DTCM.T / DTCM.T.max()\n",
    "    elif tf_method == 'binary':\n",
    "        TF = DTCM.T.astype('bool').astype('int')\n",
    "    TF = TF.T\n",
    "    DF = DTCM.astype('bool').sum() \n",
    "    N = DTCM.shape[0]\n",
    "    IDF = IDF = np.log2(N / DF)\n",
    "    TFIDF = TF * IDF\n",
    "    # Extend the TFIDF function so that it also returns the DFIDF value for each term in the VOCAB.\n",
    "\n",
    "    # We can adjust ocho level here if we want to, default is 2\n",
    "    DOC = gather_docs(CORPUS, 2)\n",
    "    DOC['n_tokens'] = DOC.doc_str.apply(lambda x: len(x.split()))\n",
    "    ngram_range = (1,2)\n",
    "    n_terms = 4000\n",
    "    count_engine = CountVectorizer(\n",
    "        stop_words = 'english',\n",
    "        ngram_range = ngram_range,\n",
    "        max_features = n_terms)\n",
    "    X = count_engine.fit_transform(DOC.doc_str)\n",
    "    DTM = pd.DataFrame(X.toarray(), \n",
    "    columns=count_engine.get_feature_names_out(), \n",
    "    index=DOC.index)\n",
    "    VOCAB = DTM.sum().to_frame('n')\n",
    "    VOCAB.index.name = 'term_str'\n",
    "    VOCAB['df'] = DTM.astype(bool).sum()\n",
    "    VOCAB['dfidf'] = VOCAB.df * np.log2(len(DTM)/VOCAB.df)\n",
    "    VOCAB['dp'] = VOCAB.df / len(DTM)\n",
    "    VOCAB['di'] = np.log2(1/VOCAB.dp)\n",
    "    VOCAB['dh'] = VOCAB.dp * VOCAB.di\n",
    "    VOCAB['n_chars'] = VOCAB.apply(lambda x: len(x.name), 1)\n",
    "    VOCAB['n_tokens'] = VOCAB.apply(lambda x: len(x.name.split()), 1)\n",
    "    VOCAB.sort_index()\n",
    "\n",
    "\n",
    "    #Hopefully this join works but I'm actually not too sure this will work\n",
    "    \n",
    "    return TFIDF,VOCAB,DTM\n",
    "\n",
    "# I think I'm going to use this oneliner more.\n",
    "# get_tfidf = lambda X, agg_func='sum': (X.T / X.T.agg(agg_func)).T * (np.log2(len(X)/X.astype('bool').sum()))\n",
    "OHCO = ['book_id','chap_id', 'para_num', 'sent_num', 'token_num']\n",
    "\n",
    "bags = dict(\n",
    "    SENTS = OHCO[:4],\n",
    "    PARAS = OHCO[:3],\n",
    "    CHAPS = OHCO[:2],\n",
    "    BOOKS = OHCO[:1]\n",
    ")\n",
    "tables = BOWtoTFIDF(BOW(CORPUS,'CHAPS'),'max', CORPUS, 2)\n",
    "DTM = tables[2]\n",
    "DFIDF = tables[1]\n",
    "TFIDF = tables[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96263b3",
   "metadata": {},
   "source": [
    "## BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc0d10a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_df= BOW(CORPUS,'CHAPS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3cefe63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiIndex([( 5200,  1),\n",
       "            ( 5200,  2),\n",
       "            ( 5200,  3),\n",
       "            ( 5252,  1),\n",
       "            ( 5353,  1),\n",
       "            ( 5454,  1),\n",
       "            ( 5454,  2),\n",
       "            ( 5555,  1),\n",
       "            ( 5656,  1),\n",
       "            ( 5757,  1),\n",
       "            ( 5858,  1),\n",
       "            ( 5959,  1),\n",
       "            ( 6060,  1),\n",
       "            ( 6161,  1),\n",
       "            ( 6262,  1),\n",
       "            ( 6262,  2),\n",
       "            ( 6262,  3),\n",
       "            ( 6262,  4),\n",
       "            ( 6262,  5),\n",
       "            ( 6262,  6),\n",
       "            ( 6363,  1),\n",
       "            ( 6464,  1),\n",
       "            ( 6565,  1),\n",
       "            ( 6666,  1),\n",
       "            ( 6767,  1),\n",
       "            ( 6969,  1),\n",
       "            ( 6969,  2),\n",
       "            ( 6969,  3),\n",
       "            ( 6969,  4),\n",
       "            ( 6969,  5),\n",
       "            ( 6969,  6),\n",
       "            ( 6969,  7),\n",
       "            ( 6969,  8),\n",
       "            ( 6969,  9),\n",
       "            ( 6969, 10),\n",
       "            ( 6969, 11),\n",
       "            ( 6969, 12),\n",
       "            ( 6969, 13),\n",
       "            ( 6969, 14),\n",
       "            ( 6969, 15),\n",
       "            ( 6969, 16),\n",
       "            ( 6969, 17),\n",
       "            ( 6969, 18),\n",
       "            ( 6969, 19),\n",
       "            ( 6969, 20),\n",
       "            ( 6969, 21),\n",
       "            ( 6969, 22),\n",
       "            ( 6969, 23),\n",
       "            ( 6969, 24),\n",
       "            ( 6969, 25),\n",
       "            ( 7849,  1),\n",
       "            ( 7849,  2),\n",
       "            ( 7849,  3),\n",
       "            ( 7849,  4),\n",
       "            ( 7849,  5),\n",
       "            ( 7849,  6),\n",
       "            ( 7849,  7),\n",
       "            ( 7849,  8),\n",
       "            ( 7849,  9),\n",
       "            ( 7849, 10),\n",
       "            (23532,  1),\n",
       "            (23532,  2),\n",
       "            (23532,  3),\n",
       "            (23532,  4),\n",
       "            (23532,  5),\n",
       "            (23532,  6),\n",
       "            (23532,  7),\n",
       "            (23532,  8),\n",
       "            (23532,  9),\n",
       "            (23532, 10)],\n",
       "           names=['book_id', 'chap_id'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TFIDF.T.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7423913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Reset index so book_id and chap_id become columns\n",
    "tfidf_reset = TFIDF.reset_index()\n",
    "\n",
    "# Step 2: Melt all term columns into a long format\n",
    "tfidf_long = tfidf_reset.melt(\n",
    "    id_vars=['book_id', 'chap_id'],\n",
    "    var_name='term_str',\n",
    "    value_name='tfidf'\n",
    ")\n",
    "\n",
    "# Step 3: Merge with bag-of-words dataframe\n",
    "bow_with_tfidf = bow_df.merge(\n",
    "    tfidf_long,\n",
    "    on=['book_id', 'chap_id', 'term_str'],\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fef9a5a",
   "metadata": {},
   "source": [
    "### BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ac74b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bow_with_tfidf.to_csv('BOW.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "33d945dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_id</th>\n",
       "      <th>chap_id</th>\n",
       "      <th>term_str</th>\n",
       "      <th>n</th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5200</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>108</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5200</td>\n",
       "      <td>1</td>\n",
       "      <td>abandoning</td>\n",
       "      <td>1</td>\n",
       "      <td>0.009230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5200</td>\n",
       "      <td>1</td>\n",
       "      <td>able</td>\n",
       "      <td>7</td>\n",
       "      <td>0.005868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5200</td>\n",
       "      <td>1</td>\n",
       "      <td>about</td>\n",
       "      <td>16</td>\n",
       "      <td>0.008273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5200</td>\n",
       "      <td>1</td>\n",
       "      <td>above</td>\n",
       "      <td>2</td>\n",
       "      <td>0.003155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80544</th>\n",
       "      <td>23532</td>\n",
       "      <td>10</td>\n",
       "      <td>you</td>\n",
       "      <td>55</td>\n",
       "      <td>0.068376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80545</th>\n",
       "      <td>23532</td>\n",
       "      <td>10</td>\n",
       "      <td>your</td>\n",
       "      <td>7</td>\n",
       "      <td>0.024125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80546</th>\n",
       "      <td>23532</td>\n",
       "      <td>10</td>\n",
       "      <td>youre</td>\n",
       "      <td>8</td>\n",
       "      <td>0.078501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80547</th>\n",
       "      <td>23532</td>\n",
       "      <td>10</td>\n",
       "      <td>yourself</td>\n",
       "      <td>2</td>\n",
       "      <td>0.018776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80548</th>\n",
       "      <td>23532</td>\n",
       "      <td>10</td>\n",
       "      <td>youve</td>\n",
       "      <td>1</td>\n",
       "      <td>0.014214</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80549 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       book_id  chap_id    term_str    n     tfidf\n",
       "0         5200        1           a  108  0.000000\n",
       "1         5200        1  abandoning    1  0.009230\n",
       "2         5200        1        able    7  0.005868\n",
       "3         5200        1       about   16  0.008273\n",
       "4         5200        1       above    2  0.003155\n",
       "...        ...      ...         ...  ...       ...\n",
       "80544    23532       10         you   55  0.068376\n",
       "80545    23532       10        your    7  0.024125\n",
       "80546    23532       10       youre    8  0.078501\n",
       "80547    23532       10    yourself    2  0.018776\n",
       "80548    23532       10       youve    1  0.014214\n",
       "\n",
       "[80549 rows x 5 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_with_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07e7904",
   "metadata": {},
   "source": [
    "## DTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "39bb5bae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term_str</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abandoning</th>\n",
       "      <th>abilities</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>able make</th>\n",
       "      <th>abruptly</th>\n",
       "      <th>absence</th>\n",
       "      <th>absent</th>\n",
       "      <th>absolute</th>\n",
       "      <th>...</th>\n",
       "      <th>ﬁre</th>\n",
       "      <th>ﬁre brigade</th>\n",
       "      <th>ﬁre engine</th>\n",
       "      <th>ﬁrmly</th>\n",
       "      <th>ﬁrst</th>\n",
       "      <th>ﬁrst time</th>\n",
       "      <th>ﬁst</th>\n",
       "      <th>ﬁt</th>\n",
       "      <th>ﬁxed</th>\n",
       "      <th>ﬂoor</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>book_id</th>\n",
       "      <th>chap_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">5200</th>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5252</th>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5353</th>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">23532</th>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70 rows × 4000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "term_str         abandoned  abandoning  abilities  ability  able  able make  \\\n",
       "book_id chap_id                                                               \n",
       "5200    1                0           1          0        0     7          0   \n",
       "        2                1           1          0        1    13          0   \n",
       "        3                1           0          0        0     3          0   \n",
       "5252    1                1           1          0        0     4          0   \n",
       "5353    1                0           0          1        1     5          0   \n",
       "...                    ...         ...        ...      ...   ...        ...   \n",
       "23532   6                0           0          0        0     0          0   \n",
       "        7                0           0          0        0     0          0   \n",
       "        8                0           0          0        0     1          0   \n",
       "        9                0           0          0        0     1          0   \n",
       "        10               1           0          0        0     1          0   \n",
       "\n",
       "term_str         abruptly  absence  absent  absolute  ...  ﬁre  ﬁre brigade  \\\n",
       "book_id chap_id                                       ...                     \n",
       "5200    1               0        0       0         0  ...    0            0   \n",
       "        2               0        0       0         0  ...    0            0   \n",
       "        3               1        0       0         0  ...    0            0   \n",
       "5252    1               0        1       0         0  ...    0            0   \n",
       "5353    1               0        6       0         0  ...    0            0   \n",
       "...                   ...      ...     ...       ...  ...  ...          ...   \n",
       "23532   6               0        0       0         0  ...    0            0   \n",
       "        7               0        0       0         0  ...    0            0   \n",
       "        8               0        0       0         0  ...    0            0   \n",
       "        9               0        0       0         0  ...    0            0   \n",
       "        10              0        0       0         0  ...    0            0   \n",
       "\n",
       "term_str         ﬁre engine  ﬁrmly  ﬁrst  ﬁrst time  ﬁst  ﬁt  ﬁxed  ﬂoor  \n",
       "book_id chap_id                                                           \n",
       "5200    1                 0      0     0          0    0   0     0     0  \n",
       "        2                 0      0     0          0    0   0     0     0  \n",
       "        3                 0      0     0          0    0   0     0     0  \n",
       "5252    1                 0      0     0          0    0   0     0     0  \n",
       "5353    1                 0      0     0          0    0   0     0     0  \n",
       "...                     ...    ...   ...        ...  ...  ..   ...   ...  \n",
       "23532   6                 0      0     0          0    0   0     0     0  \n",
       "        7                 0      0     0          0    0   0     0     0  \n",
       "        8                 0      0     0          0    0   0     0     0  \n",
       "        9                 0      0     0          0    0   0     0     0  \n",
       "        10                0      0     0          0    0   0     0     0  \n",
       "\n",
       "[70 rows x 4000 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e9210f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DTM.to_csv('DTM.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2124f6",
   "metadata": {},
   "source": [
    "## TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "495b8d39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term_str</th>\n",
       "      <th>1</th>\n",
       "      <th>10</th>\n",
       "      <th>15</th>\n",
       "      <th>2</th>\n",
       "      <th>25</th>\n",
       "      <th>3</th>\n",
       "      <th>430</th>\n",
       "      <th>536</th>\n",
       "      <th>6</th>\n",
       "      <th>a</th>\n",
       "      <th>...</th>\n",
       "      <th>ﬂoors</th>\n",
       "      <th>ﬂopped</th>\n",
       "      <th>ﬂowered</th>\n",
       "      <th>ﬂowers</th>\n",
       "      <th>ﬂown</th>\n",
       "      <th>ﬂung</th>\n",
       "      <th>ﬂushed</th>\n",
       "      <th>ﬂuttered</th>\n",
       "      <th>ﬂuttering</th>\n",
       "      <th>ﬂying</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>book_id</th>\n",
       "      <th>chap_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">5200</th>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5252</th>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5353</th>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">23532</th>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70 rows × 14598 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "term_str           1   10   15    2   25    3  430  536    6    a  ...  ﬂoors  \\\n",
       "book_id chap_id                                                    ...          \n",
       "5200    1        0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
       "        2        0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
       "        3        0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
       "5252    1        0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
       "5353    1        0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
       "...              ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...    ...   \n",
       "23532   6        0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
       "        7        0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
       "        8        0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
       "        9        0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
       "        10       0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
       "\n",
       "term_str         ﬂopped  ﬂowered  ﬂowers  ﬂown  ﬂung  ﬂushed  ﬂuttered  \\\n",
       "book_id chap_id                                                          \n",
       "5200    1           0.0      0.0     0.0   0.0   0.0     0.0       0.0   \n",
       "        2           0.0      0.0     0.0   0.0   0.0     0.0       0.0   \n",
       "        3           0.0      0.0     0.0   0.0   0.0     0.0       0.0   \n",
       "5252    1           0.0      0.0     0.0   0.0   0.0     0.0       0.0   \n",
       "5353    1           0.0      0.0     0.0   0.0   0.0     0.0       0.0   \n",
       "...                 ...      ...     ...   ...   ...     ...       ...   \n",
       "23532   6           0.0      0.0     0.0   0.0   0.0     0.0       0.0   \n",
       "        7           0.0      0.0     0.0   0.0   0.0     0.0       0.0   \n",
       "        8           0.0      0.0     0.0   0.0   0.0     0.0       0.0   \n",
       "        9           0.0      0.0     0.0   0.0   0.0     0.0       0.0   \n",
       "        10          0.0      0.0     0.0   0.0   0.0     0.0       0.0   \n",
       "\n",
       "term_str         ﬂuttering  ﬂying  \n",
       "book_id chap_id                    \n",
       "5200    1              0.0    0.0  \n",
       "        2              0.0    0.0  \n",
       "        3              0.0    0.0  \n",
       "5252    1              0.0    0.0  \n",
       "5353    1              0.0    0.0  \n",
       "...                    ...    ...  \n",
       "23532   6              0.0    0.0  \n",
       "        7              0.0    0.0  \n",
       "        8              0.0    0.0  \n",
       "        9              0.0    0.0  \n",
       "        10             0.0    0.0  \n",
       "\n",
       "[70 rows x 14598 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203a0e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TFIDF.to_csv('TFIDF.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad99cf29",
   "metadata": {},
   "source": [
    "## Reduced and Normalized TFIDF_L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1b6cbe67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term_str</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abandoning</th>\n",
       "      <th>abilities</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>able make</th>\n",
       "      <th>abruptly</th>\n",
       "      <th>absence</th>\n",
       "      <th>absent</th>\n",
       "      <th>absolute</th>\n",
       "      <th>...</th>\n",
       "      <th>ﬁre</th>\n",
       "      <th>ﬁre brigade</th>\n",
       "      <th>ﬁre engine</th>\n",
       "      <th>ﬁrmly</th>\n",
       "      <th>ﬁrst</th>\n",
       "      <th>ﬁrst time</th>\n",
       "      <th>ﬁst</th>\n",
       "      <th>ﬁt</th>\n",
       "      <th>ﬁxed</th>\n",
       "      <th>ﬂoor</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>book_id</th>\n",
       "      <th>chap_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">5200</th>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008635</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.022226</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.005631</td>\n",
       "      <td>0.007872</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00680</td>\n",
       "      <td>0.037633</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.005539</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.008543</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007158</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5252</th>\n",
       "      <th>1</th>\n",
       "      <td>0.003160</td>\n",
       "      <td>0.004418</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.006498</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003593</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5353</th>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004867</td>\n",
       "      <td>0.00438</td>\n",
       "      <td>0.009324</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.024749</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">23532</th>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.047635</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.091053</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.035651</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.018329</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70 rows × 4000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "term_str         abandoned  abandoning  abilities  ability      able  \\\n",
       "book_id chap_id                                                        \n",
       "5200    1         0.000000    0.008635   0.000000  0.00000  0.022226   \n",
       "        2         0.005631    0.007872   0.000000  0.00680  0.037633   \n",
       "        3         0.005539    0.000000   0.000000  0.00000  0.008543   \n",
       "5252    1         0.003160    0.004418   0.000000  0.00000  0.006498   \n",
       "5353    1         0.000000    0.000000   0.004867  0.00438  0.009324   \n",
       "...                    ...         ...        ...      ...       ...   \n",
       "23532   6         0.000000    0.000000   0.000000  0.00000  0.000000   \n",
       "        7         0.000000    0.000000   0.000000  0.00000  0.000000   \n",
       "        8         0.000000    0.000000   0.000000  0.00000  0.047635   \n",
       "        9         0.000000    0.000000   0.000000  0.00000  0.091053   \n",
       "        10        0.035651    0.000000   0.000000  0.00000  0.018329   \n",
       "\n",
       "term_str         able make  abruptly   absence  absent  absolute  ...  ﬁre  \\\n",
       "book_id chap_id                                                   ...        \n",
       "5200    1              0.0  0.000000  0.000000     0.0       0.0  ...  0.0   \n",
       "        2              0.0  0.000000  0.000000     0.0       0.0  ...  0.0   \n",
       "        3              0.0  0.007158  0.000000     0.0       0.0  ...  0.0   \n",
       "5252    1              0.0  0.000000  0.003593     0.0       0.0  ...  0.0   \n",
       "5353    1              0.0  0.000000  0.024749     0.0       0.0  ...  0.0   \n",
       "...                    ...       ...       ...     ...       ...  ...  ...   \n",
       "23532   6              0.0  0.000000  0.000000     0.0       0.0  ...  0.0   \n",
       "        7              0.0  0.000000  0.000000     0.0       0.0  ...  0.0   \n",
       "        8              0.0  0.000000  0.000000     0.0       0.0  ...  0.0   \n",
       "        9              0.0  0.000000  0.000000     0.0       0.0  ...  0.0   \n",
       "        10             0.0  0.000000  0.000000     0.0       0.0  ...  0.0   \n",
       "\n",
       "term_str         ﬁre brigade  ﬁre engine  ﬁrmly  ﬁrst  ﬁrst time  ﬁst   ﬁt  \\\n",
       "book_id chap_id                                                              \n",
       "5200    1                0.0         0.0    0.0   0.0        0.0  0.0  0.0   \n",
       "        2                0.0         0.0    0.0   0.0        0.0  0.0  0.0   \n",
       "        3                0.0         0.0    0.0   0.0        0.0  0.0  0.0   \n",
       "5252    1                0.0         0.0    0.0   0.0        0.0  0.0  0.0   \n",
       "5353    1                0.0         0.0    0.0   0.0        0.0  0.0  0.0   \n",
       "...                      ...         ...    ...   ...        ...  ...  ...   \n",
       "23532   6                0.0         0.0    0.0   0.0        0.0  0.0  0.0   \n",
       "        7                0.0         0.0    0.0   0.0        0.0  0.0  0.0   \n",
       "        8                0.0         0.0    0.0   0.0        0.0  0.0  0.0   \n",
       "        9                0.0         0.0    0.0   0.0        0.0  0.0  0.0   \n",
       "        10               0.0         0.0    0.0   0.0        0.0  0.0  0.0   \n",
       "\n",
       "term_str         ﬁxed  ﬂoor  \n",
       "book_id chap_id              \n",
       "5200    1         0.0   0.0  \n",
       "        2         0.0   0.0  \n",
       "        3         0.0   0.0  \n",
       "5252    1         0.0   0.0  \n",
       "5353    1         0.0   0.0  \n",
       "...               ...   ...  \n",
       "23532   6         0.0   0.0  \n",
       "        7         0.0   0.0  \n",
       "        8         0.0   0.0  \n",
       "        9         0.0   0.0  \n",
       "        10        0.0   0.0  \n",
       "\n",
       "[70 rows x 4000 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_engine = TfidfTransformer(norm='l2', use_idf=True)\n",
    "X1 = tfidf_engine.fit_transform(DTM)\n",
    "L2 = pd.DataFrame(X1.toarray(), columns=DTM.columns, index=DTM.index)\n",
    "L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "39a0d0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#L2.to_csv('L2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d634d4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
