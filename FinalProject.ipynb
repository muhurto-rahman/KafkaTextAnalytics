{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "979923b5-0493-4768-ad1a-06db54f0bc7a",
   "metadata": {},
   "source": [
    "# Final Project Notebook\n",
    "\n",
    "DS 5001 Text as Data | Spring 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7046f57f-12ed-4259-be3d-60cb67b8d044",
   "metadata": {},
   "source": [
    "# Metadata\n",
    "\n",
    "- Full Name: Mustakim Muhurto Rahman\n",
    "- Userid: bur6yx \n",
    "- GitHub Repo URL:\n",
    "- UVA Box URL:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57acd11d-eb04-4bcc-b115-f205f367de49",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "The goal of the final project is for you to create a **digital analytical edition** of a corpus using the tools, practices, and perspectives youâ€™ve learning in this course. You will select a corpus that has already been digitized and transcribed, parse that into an F-compliant set of tables, and then generate and visualize the results of a series of fitted models. You will also draw some tentative conclusions regarding the linguistic, cultural, psychological, or historical features represented by your corpus. The point of the exercise is to have you work with a corpus through the entire pipeline from ingestion to interpretation. \n",
    "\n",
    "Specifically, you will acquire a collection of long-form texts and perform the following operations:\n",
    "\n",
    "- **Convert** the collection from their source formats (F0) into a set of tables that conform to the Standard Text Analytic Data Model (F2).\n",
    "- **Annotate** these tables with statistical and linguistic features using NLP libraries such as NLTK (F3).\n",
    "- **Produce** a vector representation of the corpus to generate TFIDF values to add to the TOKEN (aka CORPUS) and VOCAB tables (F4).\n",
    "- **Model** the annotated and vectorized model with tables and features derived from the application of unsupervised methods, including PCA, LDA, and word2vec (F5).\n",
    "- **Explore** your results using statistical and visual methods.\n",
    "- **Present** conclusions about patterns observed in the corpus by means of these operations.\n",
    "\n",
    "When you are finished, you will make the results of your work available in GitHub (for code) and UVA Box (for data). You will submit to Gradescope (via Canvas) a PDF version of a Jupyter notebook that contains the information listed below.\n",
    "\n",
    "# Some Details\n",
    "\n",
    "- Please fill out your answers in each task below by editing the markdown cell. \n",
    "- Replace text that asks you to insert something with the thing, i.e. replace `(INSERT IMAGE HERE)` with an image element, e.g. `![](image.png)`.\n",
    "- For URLs, just paste the raw URL directly into the text area. Don't worry about providing link labels using `[label](link)`.\n",
    "- Please do not alter the structure of the document or cell, i.e. the bulleted lists. \n",
    "- You may add explanatory paragraphs below the bulleted lists.\n",
    "- Please name your tables as they are named in each task below.\n",
    "- Tasks are indicated by headers with point values in parentheses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568b6d68-e039-4612-858b-29510eeb5365",
   "metadata": {},
   "source": [
    "# Raw Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0889de-cd53-4aa5-80b2-a2a39060776a",
   "metadata": {},
   "source": [
    "## Source Description (1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9e395a-4b0b-4ba3-9112-80c733998dbe",
   "metadata": {},
   "source": [
    "Provide a brief description of your source material, including its provenance and content. Tell us where you found it and what kind of content it contains.\n",
    "\n",
    "My source material consists of select works by Franz Kafka, primarily those that have been translated from German to English (this was a limitation for the sake of interpretability). The collection includes Kafka's major novels - *The Trial*, *The Castle* and *Amerika* - in addition to novellas such as *The Metamorphosis* and *The Judgement*, and a myriad of short stories including *A Country Doctor* and *The Hunger Artist*. The translations used were primarily by Ian Johnston, Edwin and willa Muir, and Tania and James Stern. I accessed these texts through publically availably literary archives such as Project Gutenberg, as well as other online repositories of classic literature. While an estimated 90% of Kafka's work was lost or destroyed (often by Kafka himself), what remains forms the core of my analysis on one of my favorite authors of all time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b507c1-6dc2-44f7-b74c-790d84a48e8c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Source Features (1)\n",
    "\n",
    "Add values for the following items. (Do this for all following bulleted lists.)\n",
    "\n",
    "- Source URL: https://www.gutenberg.org/ebooks/author/1735 , https://antilogicalism.com/wp-content/uploads/2017/07/kafka.pdf , https://www.kafka-online.info/works.htm \n",
    "- UVA Box URL: https://virginia.box.com/s/k6s0w9oq0vj23160dd98ifok9gnr6hy3\n",
    "- Number of raw documents: 20\n",
    "- Total size of raw documents (e.g. in MB): 2.33 MB\n",
    "- File format(s), e.g. XML, plaintext, etc.: plaintext UTF-8 encoded txt files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590e81b1-9f70-47b5-bb25-49be4e76b98b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Source Document Structure (1)\n",
    "\n",
    "Provide a brief description of the internal structure of each document. That, describe the typical elements found in document and their relation to each other. For example, a corpus of letters might be described as having a date, an addressee, a salutation, a set of content paragraphs, and closing. If they are various structures, state that.\n",
    "\n",
    "The internal structure of each document changes based on the type of work. Kafka's novels, such as *The Trial* and *The Castle*, are typically divided into chapters, which are often unnammed or simply numbers using Roman numerals. However, short stories (such as *A Country Doctor*) and novellas typically consist of a continous body of text without chapter breaks. Works like *Meditation* are structured as a collection of short stories and prose pieces, the smallest of which can be a paragraph in length, often more allegorical than narrative in nature. \n",
    "Because of this variety, we anticipate that analytical approachs like bag-of-words models face challenges when applied uniformly across the texts. To account for these differences, we may isolate texts by form - for instance, analyzing only the novels or onlt short stories when structural consistecy is needed. In other scenarions such as sentiment analysis or thematic clustering, it might make more sense to include the entire corpus regardless of structural variation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ec4c9f-e101-46fe-ac59-a35a1b148a4b",
   "metadata": {},
   "source": [
    "# Parsed and Annotated Data\n",
    "\n",
    "Parse the raw data into the three core tables of your addition: the `LIB`, `CORPUS`, and `VOCAB` tables.\n",
    "\n",
    "These tables will be stored as CSV files with header rows.\n",
    "\n",
    "You may consider using `|` as a delimitter.\n",
    "\n",
    "Provide the following information for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3ae422db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_file_path</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>chap_regex</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>book_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5200</th>\n",
       "      <td>/Users/muhur/OneDrive/Desktop/Muhurto/Data Sci...</td>\n",
       "      <td>KAFKA, FRANZ</td>\n",
       "      <td>METAMORPHOSIS</td>\n",
       "      <td>^\\s*[IVXLCM]+\\s*$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5252</th>\n",
       "      <td>/Users/muhur/OneDrive/Desktop/Muhurto/Data Sci...</td>\n",
       "      <td>KAFKA, FRANZ</td>\n",
       "      <td>BLUMFELD</td>\n",
       "      <td>NOCHAPTERS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5353</th>\n",
       "      <td>/Users/muhur/OneDrive/Desktop/Muhurto/Data Sci...</td>\n",
       "      <td>KAFKA, FRANZ</td>\n",
       "      <td>THE BURROW</td>\n",
       "      <td>NOCHAPTERS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5454</th>\n",
       "      <td>/Users/muhur/OneDrive/Desktop/Muhurto/Data Sci...</td>\n",
       "      <td>KAFKA, FRANZ</td>\n",
       "      <td>JOSEPHINE THE SONGSTRESS</td>\n",
       "      <td>NOCHAPTERS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5555</th>\n",
       "      <td>/Users/muhur/OneDrive/Desktop/Muhurto/Data Sci...</td>\n",
       "      <td>KAFKA, FRANZ</td>\n",
       "      <td>BEFORE THE LAW</td>\n",
       "      <td>NOCHAPTERS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5656</th>\n",
       "      <td>/Users/muhur/OneDrive/Desktop/Muhurto/Data Sci...</td>\n",
       "      <td>KAFKA, FRANZ</td>\n",
       "      <td>UP IN THE GALLERY</td>\n",
       "      <td>NOCHAPTERS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5757</th>\n",
       "      <td>/Users/muhur/OneDrive/Desktop/Muhurto/Data Sci...</td>\n",
       "      <td>KAFKA, FRANZ</td>\n",
       "      <td>THE HUNTER GRACCHUS</td>\n",
       "      <td>NOCHAPTERS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5858</th>\n",
       "      <td>/Users/muhur/OneDrive/Desktop/Muhurto/Data Sci...</td>\n",
       "      <td>KAFKA, FRANZ</td>\n",
       "      <td>THE GREAT WALL OF CHINA</td>\n",
       "      <td>NOCHAPTERS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5959</th>\n",
       "      <td>/Users/muhur/OneDrive/Desktop/Muhurto/Data Sci...</td>\n",
       "      <td>KAFKA, FRANZ</td>\n",
       "      <td>A REPORT FOR AN ACADEMY</td>\n",
       "      <td>NOCHAPTERS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6060</th>\n",
       "      <td>/Users/muhur/OneDrive/Desktop/Muhurto/Data Sci...</td>\n",
       "      <td>KAFKA, FRANZ</td>\n",
       "      <td>DEAREST FATHER</td>\n",
       "      <td>NOCHAPTERS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6161</th>\n",
       "      <td>/Users/muhur/OneDrive/Desktop/Muhurto/Data Sci...</td>\n",
       "      <td>KAFKA, FRANZ</td>\n",
       "      <td>THE JUDGEMENT</td>\n",
       "      <td>NOCHAPTERS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6262</th>\n",
       "      <td>/Users/muhur/OneDrive/Desktop/Muhurto/Data Sci...</td>\n",
       "      <td>KAFKA, FRANZ</td>\n",
       "      <td>AMERIKA</td>\n",
       "      <td>^\\s*\\d+\\s*$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6363</th>\n",
       "      <td>/Users/muhur/OneDrive/Desktop/Muhurto/Data Sci...</td>\n",
       "      <td>KAFKA, FRANZ</td>\n",
       "      <td>IN THE PENAL COLONY</td>\n",
       "      <td>NOCHAPTERS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6464</th>\n",
       "      <td>/Users/muhur/OneDrive/Desktop/Muhurto/Data Sci...</td>\n",
       "      <td>KAFKA, FRANZ</td>\n",
       "      <td>THE HUNGER ARTIST</td>\n",
       "      <td>NOCHAPTERS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6565</th>\n",
       "      <td>/Users/muhur/OneDrive/Desktop/Muhurto/Data Sci...</td>\n",
       "      <td>KAFKA, FRANZ</td>\n",
       "      <td>THE JACKALS AND ARABS</td>\n",
       "      <td>NOCHAPTERS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6666</th>\n",
       "      <td>/Users/muhur/OneDrive/Desktop/Muhurto/Data Sci...</td>\n",
       "      <td>KAFKA, FRANZ</td>\n",
       "      <td>A COUNTRY DOCTOR</td>\n",
       "      <td>NOCHAPTERS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6767</th>\n",
       "      <td>/Users/muhur/OneDrive/Desktop/Muhurto/Data Sci...</td>\n",
       "      <td>KAFKA, FRANZ</td>\n",
       "      <td>AN IMPERIAL MESSAGE</td>\n",
       "      <td>NOCHAPTERS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6969</th>\n",
       "      <td>/Users/muhur/OneDrive/Desktop/Muhurto/Data Sci...</td>\n",
       "      <td>KAFKA, FRANZ</td>\n",
       "      <td>THE CASTLE</td>\n",
       "      <td>^\\s*\\d+\\s*$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7849</th>\n",
       "      <td>/Users/muhur/OneDrive/Desktop/Muhurto/Data Sci...</td>\n",
       "      <td>KAFKA, FRANZ</td>\n",
       "      <td>THE TRIAL</td>\n",
       "      <td>^\\s*Chapter\\s+(?:One|Two|Three|Four|Five|Six|S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23532</th>\n",
       "      <td>/Users/muhur/OneDrive/Desktop/Muhurto/Data Sci...</td>\n",
       "      <td>KAFKA, FRANZ</td>\n",
       "      <td>MEDITATION</td>\n",
       "      <td>^(Children on the country road|Unmasking a con...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          source_file_path        author  \\\n",
       "book_id                                                                    \n",
       "5200     /Users/muhur/OneDrive/Desktop/Muhurto/Data Sci...  KAFKA, FRANZ   \n",
       "5252     /Users/muhur/OneDrive/Desktop/Muhurto/Data Sci...  KAFKA, FRANZ   \n",
       "5353     /Users/muhur/OneDrive/Desktop/Muhurto/Data Sci...  KAFKA, FRANZ   \n",
       "5454     /Users/muhur/OneDrive/Desktop/Muhurto/Data Sci...  KAFKA, FRANZ   \n",
       "5555     /Users/muhur/OneDrive/Desktop/Muhurto/Data Sci...  KAFKA, FRANZ   \n",
       "5656     /Users/muhur/OneDrive/Desktop/Muhurto/Data Sci...  KAFKA, FRANZ   \n",
       "5757     /Users/muhur/OneDrive/Desktop/Muhurto/Data Sci...  KAFKA, FRANZ   \n",
       "5858     /Users/muhur/OneDrive/Desktop/Muhurto/Data Sci...  KAFKA, FRANZ   \n",
       "5959     /Users/muhur/OneDrive/Desktop/Muhurto/Data Sci...  KAFKA, FRANZ   \n",
       "6060     /Users/muhur/OneDrive/Desktop/Muhurto/Data Sci...  KAFKA, FRANZ   \n",
       "6161     /Users/muhur/OneDrive/Desktop/Muhurto/Data Sci...  KAFKA, FRANZ   \n",
       "6262     /Users/muhur/OneDrive/Desktop/Muhurto/Data Sci...  KAFKA, FRANZ   \n",
       "6363     /Users/muhur/OneDrive/Desktop/Muhurto/Data Sci...  KAFKA, FRANZ   \n",
       "6464     /Users/muhur/OneDrive/Desktop/Muhurto/Data Sci...  KAFKA, FRANZ   \n",
       "6565     /Users/muhur/OneDrive/Desktop/Muhurto/Data Sci...  KAFKA, FRANZ   \n",
       "6666     /Users/muhur/OneDrive/Desktop/Muhurto/Data Sci...  KAFKA, FRANZ   \n",
       "6767     /Users/muhur/OneDrive/Desktop/Muhurto/Data Sci...  KAFKA, FRANZ   \n",
       "6969     /Users/muhur/OneDrive/Desktop/Muhurto/Data Sci...  KAFKA, FRANZ   \n",
       "7849     /Users/muhur/OneDrive/Desktop/Muhurto/Data Sci...  KAFKA, FRANZ   \n",
       "23532    /Users/muhur/OneDrive/Desktop/Muhurto/Data Sci...  KAFKA, FRANZ   \n",
       "\n",
       "                            title  \\\n",
       "book_id                             \n",
       "5200                METAMORPHOSIS   \n",
       "5252                     BLUMFELD   \n",
       "5353                   THE BURROW   \n",
       "5454     JOSEPHINE THE SONGSTRESS   \n",
       "5555               BEFORE THE LAW   \n",
       "5656            UP IN THE GALLERY   \n",
       "5757          THE HUNTER GRACCHUS   \n",
       "5858      THE GREAT WALL OF CHINA   \n",
       "5959      A REPORT FOR AN ACADEMY   \n",
       "6060               DEAREST FATHER   \n",
       "6161                THE JUDGEMENT   \n",
       "6262                      AMERIKA   \n",
       "6363          IN THE PENAL COLONY   \n",
       "6464            THE HUNGER ARTIST   \n",
       "6565        THE JACKALS AND ARABS   \n",
       "6666             A COUNTRY DOCTOR   \n",
       "6767          AN IMPERIAL MESSAGE   \n",
       "6969                   THE CASTLE   \n",
       "7849                    THE TRIAL   \n",
       "23532                  MEDITATION   \n",
       "\n",
       "                                                chap_regex  \n",
       "book_id                                                     \n",
       "5200                                     ^\\s*[IVXLCM]+\\s*$  \n",
       "5252                                            NOCHAPTERS  \n",
       "5353                                            NOCHAPTERS  \n",
       "5454                                            NOCHAPTERS  \n",
       "5555                                            NOCHAPTERS  \n",
       "5656                                            NOCHAPTERS  \n",
       "5757                                            NOCHAPTERS  \n",
       "5858                                            NOCHAPTERS  \n",
       "5959                                            NOCHAPTERS  \n",
       "6060                                            NOCHAPTERS  \n",
       "6161                                            NOCHAPTERS  \n",
       "6262                                           ^\\s*\\d+\\s*$  \n",
       "6363                                            NOCHAPTERS  \n",
       "6464                                            NOCHAPTERS  \n",
       "6565                                            NOCHAPTERS  \n",
       "6666                                            NOCHAPTERS  \n",
       "6767                                            NOCHAPTERS  \n",
       "6969                                           ^\\s*\\d+\\s*$  \n",
       "7849     ^\\s*Chapter\\s+(?:One|Two|Three|Four|Five|Six|S...  \n",
       "23532    ^(Children on the country road|Unmasking a con...  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from glob import glob\n",
    "    import re\n",
    "    import nltk\n",
    "    import plotly_express as px\n",
    "    import configparser\n",
    "    import os\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(\"../../../env-sample.ini\")\n",
    "    data_home = '/Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/KafkaFinal/data'\n",
    "    output_dir = '/Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/KafkaFinal/output'\n",
    "    local_lib = '/Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/DS5001-2025-01-R/lessons/lib'\n",
    "    import sys\n",
    "    sys.path.append(local_lib)\n",
    "    from textparser import TextParser\n",
    "\n",
    "    clip_pats = [\n",
    "        r\"\\*\\*\\*\\s*START OF\",\n",
    "        r\"\\*\\*\\*\\s*END OF\"\n",
    "    ]\n",
    "\n",
    "    # All are 'chap'and 'm'\n",
    "    roman = '[IVXLCM]+'\n",
    "    caps = \"[A-Z';, -]+\"\n",
    "    ohco_pat_list = [\n",
    "        (5200,   rf\"^\\s*CHAPTER\\s+{roman}\\s*$\"), #Metamorphosis\n",
    "        (7849,   rf\"^\\s*{roman}\\s*$\"), #The Trial\n",
    "        (6969,  rf\"^\\s*LETTER .* to .*$\"), # The Castle\n",
    "        (6262,   rf\"^CHAPTER\\s+{roman}$\"), # Amerika\n",
    "        (6161,   rf\"^CHAPTER\\s+\\d+$\"), # The Judgement\n",
    "        (6060,   rf\"^Chapter\\s+\\d+$\"), # Dearest Father\n",
    "        (6363,  rf\"^Chapter\\s+\\d+$\"), # In the Penal colony\n",
    "        (6464,   rf\"^CHAPTER\\s+\\d+$\"), # The Hunger Artist\n",
    "        (6565, rf\"^\\s*CHAPTER\\s+{roman}\\.\"), # The Jackals and Arabs\n",
    "        (6666, rf\"^\\s*CHAPTER\\s+{roman}\\s*$\"), # A Country Doctor\n",
    "        (6767, rf\"^\\s*CHAPTER\\s+{roman}\\s*$\"), # An Imperial Message\n",
    "        (5959,  rf\"^(?:ETYMOLOGY|EXTRACTS|CHAPTER)\"), # A report for an Academy\n",
    "        (5858,  rf\"^\\s*CHAPTER\\s+{roman}\\.\\s*$\"), # The Great Wall of China\n",
    "        (5757, rf\"^\\s*{roman}\\.\\s*$\"), # The Hunter Gracchus\n",
    "        (5656,  rf\"^\\s*{roman}\\. .*$\"), # Up in the Gallery\n",
    "        (5555, rf\"^CHAPTER\\s+{roman}\\.?$\"), # Before the Law\n",
    "        (5454, rf\"^\\s*[A-Z,;-]+\\.\\s*$\"), # Josephine the Songstress\n",
    "        (5353,  rf\"^CHAPTER \"), # The Burrow\n",
    "        (5252, rf\"^CHAPTER\\s+{roman}\\.\\s*$\"), # Blumfeld\n",
    "        (23532, rf\"Chapter\\s+{roman}\") # Meditation\n",
    "    ]\n",
    "    chapter_regexes = [\n",
    "        (5200,   rf\"^\\s*{roman}\\s*$\"),\n",
    "        (7849,   rf\"^\\s*Chapter\\s+(?:One|Two|Three|Four|Five|Six|Seven|Eight|Nine|Ten)\\s*$\"),\n",
    "        (6969,   rf\"^\\s*\\d+\\s*$\"),\n",
    "        (6262,   rf\"^\\s*\\d+\\s*$\"),\n",
    "        (6161,   \"NOCHAPTERS\"),\n",
    "        (6060,   \"NOCHAPTERS\"),\n",
    "        (6363,   \"NOCHAPTERS\"),\n",
    "        (6464,   \"NOCHAPTERS\"),\n",
    "        (6565,   \"NOCHAPTERS\"),\n",
    "        (6666,   \"NOCHAPTERS\"),\n",
    "        (6767,   \"NOCHAPTERS\"),\n",
    "        (5959,   \"NOCHAPTERS\"),\n",
    "        (5858,   \"NOCHAPTERS\"),\n",
    "        (5757,   \"NOCHAPTERS\"),\n",
    "        (5656,   \"NOCHAPTERS\"),\n",
    "        (5555,   \"NOCHAPTERS\"),\n",
    "        (5454,   \"NOCHAPTERS\"),\n",
    "        (5353,   \"NOCHAPTERS\"),\n",
    "        (5252,   \"NOCHAPTERS\"),\n",
    "        (23532,  rf\"^(Children on the country road|Unmasking a con artist|The Sudden Walk|Resolutions|The trip to the mountains|The Bachelor's Misfortune|The Merchant|Distracted Looking Out|The Way Home|The Passers-by|Passenger|Dresses|The rejection|Food for thought for gentlemen riders|The Alley Window|Desire to become an Indian|The Trees|Unhappiness)$\")  # Poem title on line 1\n",
    "    ]\n",
    "    ohco_pat_list = chapter_regexes\n",
    "    source_files = f'{data_home}'\n",
    "    source_file_list = sorted(glob(f\"{source_files}/*.*\"))\n",
    "\n",
    "    book_data = []\n",
    "    for source_file_path in source_file_list:\n",
    "        # Get the filename only, e.g. 'pg5353.txt'\n",
    "        filename = os.path.basename(source_file_path)\n",
    "        # Extract the numeric ID from the filename (remove 'pg' and '.txt')\n",
    "        book_id = int(filename.replace('pg', '').replace('.txt', ''))\n",
    "        # Use filename (without extension) as a raw title (optional: clean further)\n",
    "        book_title = filename.replace('.txt', '').replace('_', ' ')\n",
    "        # Append a tuple of (book_id, path, title)\n",
    "        book_data.append((book_id, source_file_path, book_title))\n",
    "    # Convert to DataFrame\n",
    "    LIB = pd.DataFrame(book_data, columns=['book_id', 'source_file_path', 'raw_title']) \\\n",
    "            .set_index('book_id') \\\n",
    "            .sort_index()\n",
    "    book_titles = {\n",
    "        5200: \"Metamorphosis\",\n",
    "        7849: \"The Trial\",\n",
    "        6969: \"The Castle\",\n",
    "        6262: \"Amerika\",\n",
    "        6161: \"The Judgement\",\n",
    "        6060: \"Dearest Father\",\n",
    "        6363: \"In the Penal Colony\",\n",
    "        6464: \"The Hunger Artist\",\n",
    "        6565: \"The Jackals and Arabs\",\n",
    "        6666: \"A Country Doctor\",\n",
    "        6767: \"An Imperial Message\",\n",
    "        5959: \"A Report for an Academy\",\n",
    "        5858: \"The Great Wall of China\",\n",
    "        5757: \"The Hunter Gracchus\",\n",
    "        5656: \"Up in the Gallery\",\n",
    "        5555: \"Before the Law\",\n",
    "        5454: \"Josephine the Songstress\",\n",
    "        5353: \"The Burrow\",\n",
    "        5252: \"Blumfeld\",\n",
    "        23532: \"Meditation\"\n",
    "    }\n",
    "    book_titles = {f'pg{key}': value for key, value in book_titles.items()}\n",
    "    try:\n",
    "        LIB['author'] = 'KAFKA, FRANZ'\n",
    "        LIB['title'] = LIB.raw_title.replace(book_titles).str.upper()\n",
    "        LIB = LIB.drop('raw_title', axis=1)\n",
    "    except AttributeError:\n",
    "        pass\n",
    "    LIB['chap_regex'] = LIB.index.map(pd.Series({x[0]:x[1] for x in ohco_pat_list}))\n",
    "    LIB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "46c0af0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing 5200 METAMORPHOSIS\n",
      "Importing  /Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/KafkaFinal/data\\pg5200.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone ^\\s*[IVXLCM]+\\s*$\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n",
      "Tokenizing 5252 BLUMFELD\n",
      "Importing  /Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/KafkaFinal/data\\pg5252.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone NOCHAPTERS\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n",
      "Tokenizing 5353 THE BURROW\n",
      "Importing  /Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/KafkaFinal/data\\pg5353.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone NOCHAPTERS\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n",
      "Tokenizing 5454 JOSEPHINE THE SONGSTRESS\n",
      "Importing  /Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/KafkaFinal/data\\pg5454.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone NOCHAPTERS\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n",
      "Tokenizing 5555 BEFORE THE LAW\n",
      "Importing  /Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/KafkaFinal/data\\pg5555.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone NOCHAPTERS\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n",
      "Tokenizing 5656 UP IN THE GALLERY\n",
      "Importing  /Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/KafkaFinal/data\\pg5656.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone NOCHAPTERS\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n",
      "Tokenizing 5757 THE HUNTER GRACCHUS\n",
      "Importing  /Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/KafkaFinal/data\\pg5757.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone NOCHAPTERS\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n",
      "Tokenizing 5858 THE GREAT WALL OF CHINA\n",
      "Importing  /Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/KafkaFinal/data\\pg5858.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone NOCHAPTERS\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n",
      "Tokenizing 5959 A REPORT FOR AN ACADEMY\n",
      "Importing  /Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/KafkaFinal/data\\pg5959.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone NOCHAPTERS\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n",
      "Tokenizing 6060 DEAREST FATHER\n",
      "Importing  /Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/KafkaFinal/data\\pg6060.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone NOCHAPTERS\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n",
      "Tokenizing 6161 THE JUDGEMENT\n",
      "Importing  /Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/KafkaFinal/data\\pg6161.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone NOCHAPTERS\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n",
      "Tokenizing 6262 AMERIKA\n",
      "Importing  /Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/KafkaFinal/data\\pg6262.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone ^\\s*\\d+\\s*$\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n",
      "Tokenizing 6363 IN THE PENAL COLONY\n",
      "Importing  /Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/KafkaFinal/data\\pg6363.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone NOCHAPTERS\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n",
      "Tokenizing 6464 THE HUNGER ARTIST\n",
      "Importing  /Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/KafkaFinal/data\\pg6464.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone NOCHAPTERS\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n",
      "Tokenizing 6565 THE JACKALS AND ARABS\n",
      "Importing  /Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/KafkaFinal/data\\pg6565.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone NOCHAPTERS\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n",
      "Tokenizing 6666 A COUNTRY DOCTOR\n",
      "Importing  /Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/KafkaFinal/data\\pg6666.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone NOCHAPTERS\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n",
      "Tokenizing 6767 AN IMPERIAL MESSAGE\n",
      "Importing  /Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/KafkaFinal/data\\pg6767.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone NOCHAPTERS\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n",
      "Tokenizing 6969 THE CASTLE\n",
      "Importing  /Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/KafkaFinal/data\\pg6969.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone ^\\s*\\d+\\s*$\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n",
      "Tokenizing 7849 THE TRIAL\n",
      "Importing  /Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/KafkaFinal/data\\pg7849.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone ^\\s*Chapter\\s+(?:One|Two|Three|Four|Five|Six|Seven|Eight|Nine|Ten)\\s*$\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n",
      "Tokenizing 23532 MEDITATION\n",
      "Importing  /Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/KafkaFinal/data\\pg23532.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone ^(Children on the country road|Unmasking a con artist|The Sudden Walk|Resolutions|The trip to the mountains|The Bachelor's Misfortune|The Merchant|Distracted Looking Out|The Way Home|The Passers-by|Passenger|Dresses|The rejection|Food for thought for gentlemen riders|The Alley Window|Desire to become an Indian|The Trees|Unhappiness)$\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/DS5001-2025-01-R/lessons/lib\\textparser.py:132: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  div_lines = self.TOKENS[src_col].str.contains(div_pat, regex=True, case=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# This cell takes 16 seconds to run\n",
    "def tokenize_collection(LIB):\n",
    "\n",
    "    clip_pats = [\n",
    "        r\"\\*\\*\\*\\s*START OF\",\n",
    "        r\"\\*\\*\\*\\s*END OF\"\n",
    "    ]\n",
    "\n",
    "    books = []\n",
    "    for book_id in LIB.index:\n",
    "\n",
    "        # Announce\n",
    "        print(\"Tokenizing\", book_id, LIB.loc[book_id].title)\n",
    "\n",
    "        # Define vars\n",
    "        chap_regex = LIB.loc[book_id].chap_regex\n",
    "        ohco_pats = [('chap', chap_regex, 'm')]\n",
    "        src_file_path = LIB.loc[book_id].source_file_path\n",
    "\n",
    "        # Create object\n",
    "        text = TextParser(src_file_path, ohco_pats=ohco_pats, clip_pats=clip_pats, use_nltk=True)\n",
    "        # text = TextImporter(src_file_path, ohco_pats=ohco_pats, clip_pats=clip_pats) \n",
    "\n",
    "        # Define parameters\n",
    "        text.verbose = True\n",
    "        text.strip_hyphens = True\n",
    "        text.strip_whitespace = True\n",
    "\n",
    "        # Parse\n",
    "        text.import_source().parse_tokens();\n",
    "\n",
    "        # Name things\n",
    "        text.TOKENS['book_id'] = book_id\n",
    "        text.TOKENS = text.TOKENS.reset_index().set_index(['book_id'] + text.OHCO)\n",
    "\n",
    "        # Add to list\n",
    "        books.append(text.TOKENS)\n",
    "        \n",
    "    # Combine into a single dataframe\n",
    "    CORPUS = pd.concat(books).sort_index()\n",
    "\n",
    "    # Clean up\n",
    "    del(books)\n",
    "    del(text)\n",
    "        \n",
    "    print(\"Done\")\n",
    "        \n",
    "    return CORPUS\n",
    "CORPUS = tokenize_collection(LIB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d05ce4-ac5c-43ea-a07b-c4626338f80e",
   "metadata": {},
   "source": [
    "## LIB (2)\n",
    "\n",
    "The source documents the corpus comprises. These may be books, plays, newspaper articles, abstracts, blog posts, etc. \n",
    "\n",
    "Note that these are *not* documents in the sense used to describe a bag-of-words representation of a text, e.g. chapter.\n",
    "\n",
    "- UVA Box URL:\n",
    "- GitHub URL for notebook used to create:\n",
    "- Delimitter: \n",
    "- Number of observations:\n",
    "- List of features, including at least three that may be used for model summarization (e.g. date, author, etc.):\n",
    "- Average length of each document in characters: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304204a5-00be-46ad-b98b-0d10a9c8ca4b",
   "metadata": {},
   "source": [
    "## CORPUS (2)\n",
    "\n",
    "The sequence of word tokens in the corpus, indexed by their location in the corpus and document structures.\n",
    "\n",
    "- UVA Box URL:\n",
    "- GitHub URL for notebook used to create:\n",
    "- Delimitter:\n",
    "- Number of observations Between (should be >= 500,000 and <= 2,000,000 observations.):\n",
    "- OHCO Structure (as delimitted column names):\n",
    "- Columns (as delimitted column names, including `token_str`, `term_str`, `pos`, and `pos_group`):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae3214e-e6dd-42d6-842f-555d0058986e",
   "metadata": {},
   "source": [
    "## VOCAB (2)\n",
    "\n",
    "The unique word types (terms) in the corpus.\n",
    "\n",
    "- UVA Box URL:\n",
    "- GitHub URL for notebook used to create:\n",
    "- Delimitter:\n",
    "- Number of observations:\n",
    "- Columns (as delimitted names, including `n`, `p`', `i`, `dfidf`, `porter_stem`, `max_pos` and `max_pos_group`, `stop`):\n",
    "- Note: Your VOCAB may contain ngrams. If so, add a feature for `ngram_length`.\n",
    "- List the top 20 significant words in the corpus by DFIDF.\n",
    "\n",
    "(INSERT LIST HERE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40dabdc-baae-4408-95bc-2f735824d59b",
   "metadata": {},
   "source": [
    "# Derived Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f2ef9c-1cb5-41e8-a5ee-1e37428b4539",
   "metadata": {},
   "source": [
    "## BOW (3)\n",
    "\n",
    "A bag-of-words representation of the CORPUS.\n",
    "\n",
    "- UVA Box URL:\n",
    "- GitHub URL for notebook used to create:\n",
    "- Delimitter:\n",
    "- Bag (expressed in terms of OHCO levels):\n",
    "- Number of observations:\n",
    "- Columns (as delimitted names, including `n`, `tfidf`):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29890d2f-bf96-43ad-8d08-792393830163",
   "metadata": {
    "tags": []
   },
   "source": [
    "## DTM (3)\n",
    "\n",
    "A represenation of the BOW as a sparse count matrix.\n",
    "\n",
    "- UVA Box URL:\n",
    "- UVA Box URL of BOW used to generate (if applicable):\n",
    "- GitHub URL for notebook used to create:\n",
    "- Delimitter:\n",
    "- Bag (expressed in terms of OHCO levels):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4b4774-7c76-401d-a9de-2704f28a0821",
   "metadata": {},
   "source": [
    "## TFIDF (3)\n",
    "\n",
    "A Document-Term matrix with TFIDF values.\n",
    "\n",
    "- UVA Box URL:\n",
    "- UVA Box URL of DTM or BOW used to create:\n",
    "- GitHub URL for notebook used to create:\n",
    "- Delimitter:\n",
    "- Description of TFIDIF formula ($\\LaTeX$ OK):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd34f5ca-5361-4701-b9dd-9da66859b40b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Reduced and Normalized TFIDF_L2 (3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c548dd2-f692-4365-936c-39c84df79b90",
   "metadata": {
    "tags": []
   },
   "source": [
    "A Document-Term matrix with L2 normalized TFIDF values.\n",
    "\n",
    "- UVA Box URL:\n",
    "- UVA Box URL of source TFIDF table:\n",
    "- GitHub URL for notebook used to create:\n",
    "- Delimitter:\n",
    "- Number of features (i.e. significant words):\n",
    "- Principle of significant word selection:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c50da94-af36-4e8d-b1a7-24dbcf431880",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df79264-dd93-4199-be38-db31579b7ce8",
   "metadata": {},
   "source": [
    "## PCA Components (4)\n",
    "\n",
    "- UVA Box URL:\n",
    "- UVA Box URL of the source TFIDF_L2 table:\n",
    "- GitHub URL for notebook used to create:\n",
    "- Delimitter:\n",
    "- Number of components:\n",
    "- Library used to generate:\n",
    "- Top 5 positive terms for first component:\n",
    "- Top 5 negative terms for second component:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73adc882-cbce-4d24-9923-5d36ac609f43",
   "metadata": {
    "tags": []
   },
   "source": [
    "## PCA DCM (4)\n",
    "\n",
    "The document-component matrix generated.\n",
    "\n",
    "- UVA Box URL:\n",
    "- GitHub URL for notebook used to create:\n",
    "- Delimitter:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fd2a4a-7f2f-4259-a5c4-063168cb1b14",
   "metadata": {
    "tags": []
   },
   "source": [
    "## PCA Loadings (4)\n",
    "\n",
    "The component-term matrix generated.\n",
    "\n",
    "- UVA Box URL:\n",
    "- GitHub URL for notebook used to create:\n",
    "- Delimitter:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fff42f-6665-4941-ba3d-034627dc0124",
   "metadata": {},
   "source": [
    "## PCA Visualization 1 (4)\n",
    "\n",
    "Include a scatterplot of documents in the space created by the first two components.\n",
    "\n",
    "Color the points based on a metadata feature associated with the documents.\n",
    "\n",
    "Also include a scatterplot of the loadings for the same two components. (This does not need a feature mapped onto color.)\n",
    "\n",
    "(INSERT IMAGE HERE)\n",
    "\n",
    "(INSERT IMAGE HERE)\n",
    "\n",
    "Briefly describe the nature of the polarity you see in the first component:\n",
    "\n",
    "(INSERT DESCRIPTION HERE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb54565-7669-4a2f-90b2-a4c283277c02",
   "metadata": {},
   "source": [
    "## PCA Visualization 2 (4)\n",
    "\n",
    "Include a scatterplot of documents in the space created by the second two components.\n",
    "\n",
    "Color the points based on a metadata feature associated with the documents.\n",
    "\n",
    "Also include a scatterplot of the loadings for the same two components. (This does not need a feature mapped onto color.)\n",
    "\n",
    "(INSERT IMAGE HERE)\n",
    "\n",
    "(INSERT IMAGE HERE)\n",
    "\n",
    "Briefly describe the nature of the polarity you see in the second component:\n",
    "\n",
    "(INSERT DESCRIPTION HERE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ee23b2-25d1-4226-bf31-1607e5ed4677",
   "metadata": {
    "tags": []
   },
   "source": [
    "## LDA TOPIC (4)\n",
    "\n",
    "- UVA Box URL:\n",
    "- UVA Box URL of count matrix used to create:\n",
    "- GitHub URL for notebook used to create:\n",
    "- Delimitter:\n",
    "- Libary used to compute:\n",
    "- A description of any filtering, e.g. POS (Nouns and Verbs only):\n",
    "- Number of components:\n",
    "- Any other parameters used:\n",
    "- Top 5 words and best-guess labels for topic five topics by mean document weight:\n",
    "  - T00:\n",
    "  - T01:\n",
    "  - T02:\n",
    "  - T03:\n",
    "  - T04:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a518d520-4a5c-48fa-836d-f8ea3e3c2f06",
   "metadata": {
    "tags": []
   },
   "source": [
    "## LDA THETA (4)\n",
    "\n",
    "- UVA Box URL:\n",
    "- GitHub URL for notebook used to create:\n",
    "- Delimitter:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8808b30-64f4-4249-95d5-d7c0925ce432",
   "metadata": {
    "tags": []
   },
   "source": [
    "## LDA PHI (4)\n",
    "\n",
    "- UVA Box URL:\n",
    "- GitHub URL for notebook used to create:\n",
    "- Delimitter:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e404bf-8a2a-4eb4-ba89-0c708c8f359d",
   "metadata": {},
   "source": [
    "## LDA + PCA Visualization (4)\n",
    "\n",
    "Apply PCA to the PHI table and plot the topics in the space opened by the first two components.\n",
    "\n",
    "Size the points based on the mean document weight of each topic (using the THETA table).\n",
    "\n",
    "Color the points basd on a metadata feature from the LIB table.\n",
    "\n",
    "Provide a brief interpretation of what you see.\n",
    "\n",
    "(INSERT IMAGE HERE)\n",
    "\n",
    "(INSERT INTERPRETATION HERE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e1f327-a386-476a-8d94-2ab7a63afa7a",
   "metadata": {},
   "source": [
    "## Sentiment VOCAB_SENT (4)\n",
    "\n",
    "Sentiment values associated with a subset of the VOCAB from a curated sentiment lexicon.\n",
    "\n",
    "- UVA Box URL:\n",
    "- UVA Box URL for source lexicon:\n",
    "- GitHub URL for notebook used to create:\n",
    "- Delimitter:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8a9d67-1560-4be9-b82a-b99a60b5c93e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Sentiment BOW_SENT (4)\n",
    "\n",
    "Sentiment values from VOCAB_SENT mapped onto BOW.\n",
    "\n",
    "- UVA Box URL:\n",
    "- GitHub URL for notebook used to create:\n",
    "- Delimitter:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ee6837-b12e-453d-96c1-59eaa4b28883",
   "metadata": {},
   "source": [
    "## Sentiment DOC_SENT (4)\n",
    "\n",
    "Computed sentiment per bag computed from BOW_SENT.\n",
    "\n",
    "- UVA Box URL:\n",
    "- GitHub URL for notebook used to create:\n",
    "- Delimitter:\n",
    "- Document bag expressed in terms of OHCO levels:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4cba13-e60a-4940-a06d-02479f002c3c",
   "metadata": {},
   "source": [
    "## Sentiment Plot (4)\n",
    "\n",
    "Plot sentiment over some metric space, such as time.\n",
    "\n",
    "If you don't have a metric metadata features, plot sentiment over a feature of your choice.\n",
    "\n",
    "You may use a bar chart or a line graph.\n",
    "\n",
    "(INSERT IMAGE HERE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5d2316-317b-4d95-a804-aff98242e411",
   "metadata": {},
   "source": [
    "## VOCAB_W2V (4)\n",
    "\n",
    "A table of word2vec features associated with terms in the VOCAB table.\n",
    "\n",
    "- UVA Box URL:\n",
    "- GitHub URL for notebook used to create:\n",
    "- Delimitter:\n",
    "- Document bag expressed in terms of OHCO levels:\n",
    "- Number of features generated:\n",
    "- The library used to generate the embeddings:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833c1974-047b-4285-9f4d-7f3314f39542",
   "metadata": {},
   "source": [
    "## Word2vec tSNE Plot (4)\n",
    "\n",
    "Plot word embedding featues in two-dimensions using t-SNE.\n",
    "\n",
    "Describe a cluster in the plot that captures your attention.\n",
    "\n",
    "(INSERT IMAGE HERE)\n",
    "\n",
    "(INSERT DESCRIPTION HERE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75878341-7fe8-4e22-b908-36029f9818e8",
   "metadata": {},
   "source": [
    "# Riffs\n",
    "\n",
    "Provde at least three visualizations that combine the preceding model data in interesting ways.\n",
    "\n",
    "These should provide insight into how features in the LIB table are related. \n",
    "\n",
    "The nature of this relationship is left open to you -- it may be correlation, or mutual information, or something less well defined. \n",
    "\n",
    "In doing so, consider the following visualization types:\n",
    "\n",
    "- Hierarchical cluster diagrams\n",
    "- Heatmaps\n",
    "- Scatter plots\n",
    "- KDE plots\n",
    "- Dispersion plots\n",
    "- t-SNE plots\n",
    "- etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c62acf1-6bb0-45d0-aed2-863b285f8cad",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Riff 1 (5)\n",
    "\n",
    "(INSERT IMAGE HERE)\n",
    "\n",
    "(INSERT INTERPRETATION HERE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2155a072-02b3-4aa8-b9f1-e43a59e9a85d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Riff 2 (5)\n",
    "\n",
    "(INSERT IMAGE HERE)\n",
    "\n",
    "(INSERT INTERPRETATION HERE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5067c59b-8983-4acc-972a-1ecd852ded57",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Riff 3 (5)\n",
    "\n",
    "(INSERT IMAGE HERE)\n",
    "\n",
    "(INSERT INTERPRETATION HERE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e25c6e-2624-4899-829e-e7d60c878685",
   "metadata": {},
   "source": [
    "# Interpretation (4)\n",
    "\n",
    "Describe something interesting about your corpus that you discovered during the process of completing this assignment.\n",
    "\n",
    "At a minumum, use 250 words, but you may use more. You may also add images if you'd like.\n",
    "\n",
    "(INSERT INTERPRETATION HERE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800d056b-2bca-4724-be9c-91ea640b8129",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
