{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a68ec0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing 5200 METAMORPHOSIS\n",
      "Importing  /Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/KafkaFinal/data\\pg5200.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone ^\\s*[IVXLCM]+\\s*$\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n",
      "Tokenizing 5252 BLUMFELD\n",
      "Importing  /Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/KafkaFinal/data\\pg5252.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone NOCHAPTERS\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n",
      "Tokenizing 5353 THE BURROW\n",
      "Importing  /Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/KafkaFinal/data\\pg5353.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone NOCHAPTERS\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n",
      "Tokenizing 5454 JOSEPHINE THE SONGSTRESS\n",
      "Importing  /Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/KafkaFinal/data\\pg5454.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone NOCHAPTERS\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n",
      "Tokenizing 5555 BEFORE THE LAW\n",
      "Importing  /Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/KafkaFinal/data\\pg5555.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone NOCHAPTERS\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n",
      "Tokenizing 5656 UP IN THE GALLERY\n",
      "Importing  /Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/KafkaFinal/data\\pg5656.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone NOCHAPTERS\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n",
      "Tokenizing 5757 THE HUNTER GRACCHUS\n",
      "Importing  /Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/KafkaFinal/data\\pg5757.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone NOCHAPTERS\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n",
      "Tokenizing 5858 THE GREAT WALL OF CHINA\n",
      "Importing  /Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/KafkaFinal/data\\pg5858.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone NOCHAPTERS\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n",
      "Tokenizing 5959 A REPORT FOR AN ACADEMY\n",
      "Importing  /Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/KafkaFinal/data\\pg5959.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone NOCHAPTERS\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n",
      "Tokenizing 6060 DEAREST FATHER\n",
      "Importing  /Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/KafkaFinal/data\\pg6060.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone NOCHAPTERS\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n",
      "Tokenizing 6161 THE JUDGEMENT\n",
      "Importing  /Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/KafkaFinal/data\\pg6161.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone NOCHAPTERS\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n",
      "Tokenizing 6262 AMERIKA\n",
      "Importing  /Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/KafkaFinal/data\\pg6262.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone ^\\s*\\d+\\s*$\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n",
      "Tokenizing 6363 IN THE PENAL COLONY\n",
      "Importing  /Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/KafkaFinal/data\\pg6363.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone NOCHAPTERS\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n",
      "Tokenizing 6464 THE HUNGER ARTIST\n",
      "Importing  /Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/KafkaFinal/data\\pg6464.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone NOCHAPTERS\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n",
      "Tokenizing 6565 THE JACKALS AND ARABS\n",
      "Importing  /Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/KafkaFinal/data\\pg6565.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone NOCHAPTERS\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n",
      "Tokenizing 6666 A COUNTRY DOCTOR\n",
      "Importing  /Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/KafkaFinal/data\\pg6666.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone NOCHAPTERS\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n",
      "Tokenizing 6767 AN IMPERIAL MESSAGE\n",
      "Importing  /Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/KafkaFinal/data\\pg6767.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone NOCHAPTERS\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n",
      "Tokenizing 6969 THE CASTLE\n",
      "Importing  /Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/KafkaFinal/data\\pg6969.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone ^\\s*\\d+\\s*$\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n",
      "Tokenizing 7849 THE TRIAL\n",
      "Importing  /Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/KafkaFinal/data\\pg7849.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone ^\\s*Chapter\\s+(?:One|Two|Three|Four|Five|Six|Seven|Eight|Nine|Ten)\\s*$\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n",
      "Tokenizing 23532 MEDITATION\n",
      "Importing  /Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/KafkaFinal/data\\pg23532.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone ^(Children on the country road|Unmasking a con artist|The Sudden Walk|Resolutions|The trip to the mountains|The Bachelor's Misfortune|The Merchant|Distracted Looking Out|The Way Home|The Passers-by|Passenger|Dresses|The rejection|Food for thought for gentlemen riders|The Alley Window|Desire to become an Indian|The Trees|Unhappiness)$\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/DS5001-2025-01-R/lessons/lib\\textparser.py:132: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  div_lines = self.TOKENS[src_col].str.contains(div_pat, regex=True, case=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import re\n",
    "import nltk\n",
    "import plotly_express as px\n",
    "import configparser\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import re\n",
    "import nltk\n",
    "import plotly_express as px\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "import scipy.cluster.hierarchy as sch\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.linalg import norm\n",
    "from scipy.spatial.distance import pdist\n",
    "import plotly_express as px\n",
    "import seaborn as sns; sns.set()\n",
    "import configparser\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\"../../../env-sample.ini\")\n",
    "data_home = '/Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/KafkaFinal/data'\n",
    "output_dir = '/Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/KafkaFinal/output'\n",
    "local_lib = '/Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/DS5001-2025-01-R/lessons/lib'\n",
    "import sys\n",
    "sys.path.append(local_lib)\n",
    "from textparser import TextParser\n",
    "\n",
    "clip_pats = [\n",
    "    r\"\\*\\*\\*\\s*START OF\",\n",
    "    r\"\\*\\*\\*\\s*END OF\"\n",
    "]\n",
    "\n",
    "# All are 'chap'and 'm'\n",
    "roman = '[IVXLCM]+'\n",
    "caps = \"[A-Z';, -]+\"\n",
    "ohco_pat_list = [\n",
    "    (5200,   rf\"^\\s*CHAPTER\\s+{roman}\\s*$\"), #Metamorphosis\n",
    "    (7849,   rf\"^\\s*{roman}\\s*$\"), #The Trial\n",
    "    (6969,  rf\"^\\s*LETTER .* to .*$\"), # The Castle\n",
    "    (6262,   rf\"^CHAPTER\\s+{roman}$\"), # Amerika\n",
    "    (6161,   rf\"^CHAPTER\\s+\\d+$\"), # The Judgement\n",
    "    (6060,   rf\"^Chapter\\s+\\d+$\"), # Dearest Father\n",
    "    (6363,  rf\"^Chapter\\s+\\d+$\"), # In the Penal colony\n",
    "    (6464,   rf\"^CHAPTER\\s+\\d+$\"), # The Hunger Artist\n",
    "    (6565, rf\"^\\s*CHAPTER\\s+{roman}\\.\"), # The Jackals and Arabs\n",
    "    (6666, rf\"^\\s*CHAPTER\\s+{roman}\\s*$\"), # A Country Doctor\n",
    "    (6767, rf\"^\\s*CHAPTER\\s+{roman}\\s*$\"), # An Imperial Message\n",
    "    (5959,  rf\"^(?:ETYMOLOGY|EXTRACTS|CHAPTER)\"), # A report for an Academy\n",
    "    (5858,  rf\"^\\s*CHAPTER\\s+{roman}\\.\\s*$\"), # The Great Wall of China\n",
    "    (5757, rf\"^\\s*{roman}\\.\\s*$\"), # The Hunter Gracchus\n",
    "    (5656,  rf\"^\\s*{roman}\\. .*$\"), # Up in the Gallery\n",
    "    (5555, rf\"^CHAPTER\\s+{roman}\\.?$\"), # Before the Law\n",
    "    (5454, rf\"^\\s*[A-Z,;-]+\\.\\s*$\"), # Josephine the Songstress\n",
    "    (5353,  rf\"^CHAPTER \"), # The Burrow\n",
    "    (5252, rf\"^CHAPTER\\s+{roman}\\.\\s*$\"), # Blumfeld\n",
    "    (23532, rf\"Chapter\\s+{roman}\") # Meditation\n",
    "]\n",
    "chapter_regexes = [\n",
    "    (5200,   rf\"^\\s*{roman}\\s*$\"),\n",
    "    (7849,   rf\"^\\s*Chapter\\s+(?:One|Two|Three|Four|Five|Six|Seven|Eight|Nine|Ten)\\s*$\"),\n",
    "    (6969,   rf\"^\\s*\\d+\\s*$\"),\n",
    "    (6262,   rf\"^\\s*\\d+\\s*$\"),\n",
    "    (6161,   \"NOCHAPTERS\"),\n",
    "    (6060,   \"NOCHAPTERS\"),\n",
    "    (6363,   \"NOCHAPTERS\"),\n",
    "    (6464,   \"NOCHAPTERS\"),\n",
    "    (6565,   \"NOCHAPTERS\"),\n",
    "    (6666,   \"NOCHAPTERS\"),\n",
    "    (6767,   \"NOCHAPTERS\"),\n",
    "    (5959,   \"NOCHAPTERS\"),\n",
    "    (5858,   \"NOCHAPTERS\"),\n",
    "    (5757,   \"NOCHAPTERS\"),\n",
    "    (5656,   \"NOCHAPTERS\"),\n",
    "    (5555,   \"NOCHAPTERS\"),\n",
    "    (5454,   \"NOCHAPTERS\"),\n",
    "    (5353,   \"NOCHAPTERS\"),\n",
    "    (5252,   \"NOCHAPTERS\"),\n",
    "    (23532,  rf\"^(Children on the country road|Unmasking a con artist|The Sudden Walk|Resolutions|The trip to the mountains|The Bachelor's Misfortune|The Merchant|Distracted Looking Out|The Way Home|The Passers-by|Passenger|Dresses|The rejection|Food for thought for gentlemen riders|The Alley Window|Desire to become an Indian|The Trees|Unhappiness)$\")  # Poem title on line 1\n",
    "]\n",
    "ohco_pat_list = chapter_regexes\n",
    "source_files = f'{data_home}'\n",
    "source_file_list = sorted(glob(f\"{source_files}/*.*\"))\n",
    "\n",
    "book_data = []\n",
    "for source_file_path in source_file_list:\n",
    "    # Get the filename only, e.g. 'pg5353.txt'\n",
    "    filename = os.path.basename(source_file_path)\n",
    "    # Extract the numeric ID from the filename (remove 'pg' and '.txt')\n",
    "    book_id = int(filename.replace('pg', '').replace('.txt', ''))\n",
    "    # Use filename (without extension) as a raw title (optional: clean further)\n",
    "    book_title = filename.replace('.txt', '').replace('_', ' ')\n",
    "    # Append a tuple of (book_id, path, title)\n",
    "    book_data.append((book_id, source_file_path, book_title))\n",
    "# Convert to DataFrame\n",
    "LIB = pd.DataFrame(book_data, columns=['book_id', 'source_file_path', 'raw_title']) \\\n",
    "        .set_index('book_id') \\\n",
    "        .sort_index()\n",
    "book_titles = {\n",
    "    5200: \"Metamorphosis\",\n",
    "    7849: \"The Trial\",\n",
    "    6969: \"The Castle\",\n",
    "    6262: \"Amerika\",\n",
    "    6161: \"The Judgement\",\n",
    "    6060: \"Dearest Father\",\n",
    "    6363: \"In the Penal Colony\",\n",
    "    6464: \"The Hunger Artist\",\n",
    "    6565: \"The Jackals and Arabs\",\n",
    "    6666: \"A Country Doctor\",\n",
    "    6767: \"An Imperial Message\",\n",
    "    5959: \"A Report for an Academy\",\n",
    "    5858: \"The Great Wall of China\",\n",
    "    5757: \"The Hunter Gracchus\",\n",
    "    5656: \"Up in the Gallery\",\n",
    "    5555: \"Before the Law\",\n",
    "    5454: \"Josephine the Songstress\",\n",
    "    5353: \"The Burrow\",\n",
    "    5252: \"Blumfeld\",\n",
    "    23532: \"Meditation\"\n",
    "}\n",
    "book_titles = {f'pg{key}': value for key, value in book_titles.items()}\n",
    "try:\n",
    "    LIB['author'] = 'KAFKA, FRANZ'\n",
    "    LIB['title'] = LIB.raw_title.replace(book_titles).str.upper()\n",
    "    LIB = LIB.drop('raw_title', axis=1)\n",
    "except AttributeError:\n",
    "    pass\n",
    "LIB['chap_regex'] = LIB.index.map(pd.Series({x[0]:x[1] for x in ohco_pat_list}))\n",
    "LIB\n",
    "# This cell takes 16 seconds to run\n",
    "def tokenize_collection(LIB):\n",
    "\n",
    "    clip_pats = [\n",
    "        r\"\\*\\*\\*\\s*START OF\",\n",
    "        r\"\\*\\*\\*\\s*END OF\"\n",
    "    ]\n",
    "\n",
    "    books = []\n",
    "    for book_id in LIB.index:\n",
    "\n",
    "        # Announce\n",
    "        print(\"Tokenizing\", book_id, LIB.loc[book_id].title)\n",
    "\n",
    "        # Define vars\n",
    "        chap_regex = LIB.loc[book_id].chap_regex\n",
    "        ohco_pats = [('chap', chap_regex, 'm')]\n",
    "        src_file_path = LIB.loc[book_id].source_file_path\n",
    "\n",
    "        # Create object\n",
    "        text = TextParser(src_file_path, ohco_pats=ohco_pats, clip_pats=clip_pats, use_nltk=True)\n",
    "        # text = TextImporter(src_file_path, ohco_pats=ohco_pats, clip_pats=clip_pats) \n",
    "\n",
    "        # Define parameters\n",
    "        text.verbose = True\n",
    "        text.strip_hyphens = True\n",
    "        text.strip_whitespace = True\n",
    "\n",
    "        # Parse\n",
    "        text.import_source().parse_tokens();\n",
    "\n",
    "        # Name things\n",
    "        text.TOKENS['book_id'] = book_id\n",
    "        text.TOKENS = text.TOKENS.reset_index().set_index(['book_id'] + text.OHCO)\n",
    "\n",
    "        # Add to list\n",
    "        books.append(text.TOKENS)\n",
    "        \n",
    "    # Combine into a single dataframe\n",
    "    CORPUS = pd.concat(books).sort_index()\n",
    "\n",
    "    # Clean up\n",
    "    del(books)\n",
    "    del(text)\n",
    "        \n",
    "    print(\"Done\")\n",
    "        \n",
    "    return CORPUS\n",
    "CORPUS = tokenize_collection(LIB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1d53bc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>pos_tuple</th>\n",
       "      <th>pos</th>\n",
       "      <th>token_str</th>\n",
       "      <th>term_str</th>\n",
       "      <th>pos_group</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>book_id</th>\n",
       "      <th>chap_id</th>\n",
       "      <th>para_num</th>\n",
       "      <th>sent_num</th>\n",
       "      <th>token_num</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">5200</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th>0</th>\n",
       "      <td>(One, CD)</td>\n",
       "      <td>CD</td>\n",
       "      <td>One</td>\n",
       "      <td>one</td>\n",
       "      <td>CD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(morning,, NN)</td>\n",
       "      <td>NN</td>\n",
       "      <td>morning,</td>\n",
       "      <td>morning</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(when, WRB)</td>\n",
       "      <td>WRB</td>\n",
       "      <td>when</td>\n",
       "      <td>when</td>\n",
       "      <td>WR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Gregor, NNP)</td>\n",
       "      <td>NNP</td>\n",
       "      <td>Gregor</td>\n",
       "      <td>gregor</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(Samsa, NNP)</td>\n",
       "      <td>NNP</td>\n",
       "      <td>Samsa</td>\n",
       "      <td>samsa</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">23532</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">10</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">39</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th>27</th>\n",
       "      <td>(upstairs, JJ)</td>\n",
       "      <td>JJ</td>\n",
       "      <td>upstairs</td>\n",
       "      <td>upstairs</td>\n",
       "      <td>JJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>(and, CC)</td>\n",
       "      <td>CC</td>\n",
       "      <td>and</td>\n",
       "      <td>and</td>\n",
       "      <td>CC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>(go, VB)</td>\n",
       "      <td>VB</td>\n",
       "      <td>go</td>\n",
       "      <td>go</td>\n",
       "      <td>VB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>(to, TO)</td>\n",
       "      <td>TO</td>\n",
       "      <td>to</td>\n",
       "      <td>to</td>\n",
       "      <td>TO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>(sleep., VB)</td>\n",
       "      <td>VB</td>\n",
       "      <td>sleep.</td>\n",
       "      <td>sleep</td>\n",
       "      <td>VB</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>423144 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  pos_tuple  pos token_str  \\\n",
       "book_id chap_id para_num sent_num token_num                                  \n",
       "5200    1       1        0        0               (One, CD)   CD       One   \n",
       "                                  1          (morning,, NN)   NN  morning,   \n",
       "                                  2             (when, WRB)  WRB      when   \n",
       "                                  3           (Gregor, NNP)  NNP    Gregor   \n",
       "                                  4            (Samsa, NNP)  NNP     Samsa   \n",
       "...                                                     ...  ...       ...   \n",
       "23532   10      39       0        27         (upstairs, JJ)   JJ  upstairs   \n",
       "                                  28              (and, CC)   CC       and   \n",
       "                                  29               (go, VB)   VB        go   \n",
       "                                  30               (to, TO)   TO        to   \n",
       "                                  31           (sleep., VB)   VB    sleep.   \n",
       "\n",
       "                                             term_str pos_group  \n",
       "book_id chap_id para_num sent_num token_num                      \n",
       "5200    1       1        0        0               one        CD  \n",
       "                                  1           morning        NN  \n",
       "                                  2              when        WR  \n",
       "                                  3            gregor        NN  \n",
       "                                  4             samsa        NN  \n",
       "...                                               ...       ...  \n",
       "23532   10      39       0        27         upstairs        JJ  \n",
       "                                  28              and        CC  \n",
       "                                  29               go        VB  \n",
       "                                  30               to        TO  \n",
       "                                  31            sleep        VB  \n",
       "\n",
       "[423144 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CORPUS = CORPUS[CORPUS.term_str != '']\n",
    "CORPUS['pos_group'] = CORPUS.pos.str[:2]\n",
    "CORPUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9890f086",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>n_chars</th>\n",
       "      <th>p</th>\n",
       "      <th>i</th>\n",
       "      <th>max_pos</th>\n",
       "      <th>max_pos_group</th>\n",
       "      <th>n_pos_group</th>\n",
       "      <th>cat_pos_group</th>\n",
       "      <th>n_pos</th>\n",
       "      <th>cat_pos</th>\n",
       "      <th>stop</th>\n",
       "      <th>porter_stem</th>\n",
       "      <th>stem_snowball</th>\n",
       "      <th>stem_lancaster</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>term_str</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>18.690615</td>\n",
       "      <td>JJ</td>\n",
       "      <td>JJ</td>\n",
       "      <td>1</td>\n",
       "      <td>{JJ}</td>\n",
       "      <td>1</td>\n",
       "      <td>{JJ}</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>18.690615</td>\n",
       "      <td>CD</td>\n",
       "      <td>CD</td>\n",
       "      <td>1</td>\n",
       "      <td>{CD}</td>\n",
       "      <td>1</td>\n",
       "      <td>{CD}</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>18.690615</td>\n",
       "      <td>CD</td>\n",
       "      <td>CD</td>\n",
       "      <td>1</td>\n",
       "      <td>{CD}</td>\n",
       "      <td>1</td>\n",
       "      <td>{CD}</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>18.690615</td>\n",
       "      <td>NN</td>\n",
       "      <td>NN</td>\n",
       "      <td>1</td>\n",
       "      <td>{NN}</td>\n",
       "      <td>1</td>\n",
       "      <td>{NN}</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>18.690615</td>\n",
       "      <td>CD</td>\n",
       "      <td>CD</td>\n",
       "      <td>1</td>\n",
       "      <td>{CD}</td>\n",
       "      <td>1</td>\n",
       "      <td>{CD}</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ﬂung</th>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>16.105653</td>\n",
       "      <td>NNP</td>\n",
       "      <td>NN</td>\n",
       "      <td>4</td>\n",
       "      <td>{VB, IN, JJ, NN}</td>\n",
       "      <td>5</td>\n",
       "      <td>{NN, IN, JJ, VBP, NNP}</td>\n",
       "      <td>0</td>\n",
       "      <td>ﬂung</td>\n",
       "      <td>ﬂung</td>\n",
       "      <td>ﬂung</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ﬂushed</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>18.690615</td>\n",
       "      <td>NN</td>\n",
       "      <td>NN</td>\n",
       "      <td>1</td>\n",
       "      <td>{NN}</td>\n",
       "      <td>1</td>\n",
       "      <td>{NN}</td>\n",
       "      <td>0</td>\n",
       "      <td>ﬂush</td>\n",
       "      <td>ﬂush</td>\n",
       "      <td>ﬂush</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ﬂuttered</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>18.690615</td>\n",
       "      <td>VBN</td>\n",
       "      <td>VB</td>\n",
       "      <td>1</td>\n",
       "      <td>{VB}</td>\n",
       "      <td>1</td>\n",
       "      <td>{VBN}</td>\n",
       "      <td>0</td>\n",
       "      <td>ﬂutter</td>\n",
       "      <td>ﬂutter</td>\n",
       "      <td>ﬂut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ﬂuttering</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>18.690615</td>\n",
       "      <td>VBG</td>\n",
       "      <td>VB</td>\n",
       "      <td>1</td>\n",
       "      <td>{VB}</td>\n",
       "      <td>1</td>\n",
       "      <td>{VBG}</td>\n",
       "      <td>0</td>\n",
       "      <td>ﬂutter</td>\n",
       "      <td>ﬂutter</td>\n",
       "      <td>ﬂut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ﬂying</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>17.690615</td>\n",
       "      <td>VBG</td>\n",
       "      <td>VB</td>\n",
       "      <td>1</td>\n",
       "      <td>{VB}</td>\n",
       "      <td>1</td>\n",
       "      <td>{VBG}</td>\n",
       "      <td>0</td>\n",
       "      <td>ﬂy</td>\n",
       "      <td>ﬂy</td>\n",
       "      <td>ﬂying</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14598 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           n  n_chars         p          i max_pos max_pos_group  n_pos_group  \\\n",
       "term_str                                                                        \n",
       "1          1        1  0.000002  18.690615      JJ            JJ            1   \n",
       "10         1        2  0.000002  18.690615      CD            CD            1   \n",
       "15         1        2  0.000002  18.690615      CD            CD            1   \n",
       "2          1        1  0.000002  18.690615      NN            NN            1   \n",
       "25         1        2  0.000002  18.690615      CD            CD            1   \n",
       "...       ..      ...       ...        ...     ...           ...          ...   \n",
       "ﬂung       6        4  0.000014  16.105653     NNP            NN            4   \n",
       "ﬂushed     1        6  0.000002  18.690615      NN            NN            1   \n",
       "ﬂuttered   1        8  0.000002  18.690615     VBN            VB            1   \n",
       "ﬂuttering  1        9  0.000002  18.690615     VBG            VB            1   \n",
       "ﬂying      2        5  0.000005  17.690615     VBG            VB            1   \n",
       "\n",
       "              cat_pos_group  n_pos                 cat_pos  stop porter_stem  \\\n",
       "term_str                                                                       \n",
       "1                      {JJ}      1                    {JJ}     0           1   \n",
       "10                     {CD}      1                    {CD}     0          10   \n",
       "15                     {CD}      1                    {CD}     0          15   \n",
       "2                      {NN}      1                    {NN}     0           2   \n",
       "25                     {CD}      1                    {CD}     0          25   \n",
       "...                     ...    ...                     ...   ...         ...   \n",
       "ﬂung       {VB, IN, JJ, NN}      5  {NN, IN, JJ, VBP, NNP}     0        ﬂung   \n",
       "ﬂushed                 {NN}      1                    {NN}     0        ﬂush   \n",
       "ﬂuttered               {VB}      1                   {VBN}     0      ﬂutter   \n",
       "ﬂuttering              {VB}      1                   {VBG}     0      ﬂutter   \n",
       "ﬂying                  {VB}      1                   {VBG}     0          ﬂy   \n",
       "\n",
       "          stem_snowball stem_lancaster  \n",
       "term_str                                \n",
       "1                     1              1  \n",
       "10                   10             10  \n",
       "15                   15             15  \n",
       "2                     2              2  \n",
       "25                   25             25  \n",
       "...                 ...            ...  \n",
       "ﬂung               ﬂung           ﬂung  \n",
       "ﬂushed             ﬂush           ﬂush  \n",
       "ﬂuttered         ﬂutter            ﬂut  \n",
       "ﬂuttering        ﬂutter            ﬂut  \n",
       "ﬂying                ﬂy          ﬂying  \n",
       "\n",
       "[14598 rows x 14 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB = CORPUS.term_str.value_counts().to_frame('n').sort_index()\n",
    "VOCAB.index.name = 'term_str'\n",
    "VOCAB['n_chars'] = VOCAB.index.str.len()\n",
    "VOCAB['p'] = VOCAB.n / VOCAB.n.sum()\n",
    "VOCAB['i'] = -np.log2(VOCAB.p)\n",
    "VOCAB['max_pos'] = CORPUS[['term_str','pos']].value_counts().unstack(fill_value=0).idxmax(1)\n",
    "VOCAB['max_pos_group'] = CORPUS[['term_str','pos_group']].value_counts().unstack(fill_value=0).idxmax(1)\n",
    "VOCAB['n_pos_group'] = CORPUS[['term_str','pos_group']].value_counts().unstack().count(1)\n",
    "VOCAB['cat_pos_group'] = CORPUS[['term_str','pos_group']].value_counts().to_frame('n').reset_index()\\\n",
    "    .groupby('term_str').pos_group.apply(lambda x: set(x))\n",
    "VOCAB['n_pos'] = CORPUS[['term_str','pos']].value_counts().unstack().count(1)\n",
    "VOCAB['cat_pos'] = CORPUS[['term_str','pos']].value_counts().to_frame('n').reset_index()\\\n",
    "    .groupby('term_str').pos.apply(lambda x: set(x))\n",
    "sw = pd.DataFrame(nltk.corpus.stopwords.words('english'), columns=['term_str'])\n",
    "sw = sw.reset_index().set_index('term_str')\n",
    "sw.columns = ['dummy']\n",
    "sw.dummy = 1\n",
    "VOCAB['stop'] = VOCAB.index.map(sw.dummy)\n",
    "VOCAB['stop'] = VOCAB['stop'].fillna(0).astype('int')\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer1 = PorterStemmer()\n",
    "VOCAB['porter_stem'] = VOCAB.apply(lambda x: stemmer1.stem(x.name), 1)\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer2 = SnowballStemmer(\"english\")\n",
    "VOCAB['stem_snowball'] = VOCAB.apply(lambda x: stemmer2.stem(x.name), 1)\n",
    "\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "stemmer3 = LancasterStemmer()\n",
    "VOCAB['stem_lancaster'] = VOCAB.apply(lambda x: stemmer3.stem(x.name), 1)\n",
    "\n",
    "VOCAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e4b8084",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['n',\n",
       " 'n_chars',\n",
       " 'p',\n",
       " 'i',\n",
       " 'max_pos',\n",
       " 'max_pos_group',\n",
       " 'n_pos_group',\n",
       " 'cat_pos_group',\n",
       " 'n_pos',\n",
       " 'cat_pos',\n",
       " 'stop',\n",
       " 'porter_stem',\n",
       " 'stem_snowball',\n",
       " 'stem_lancaster']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8da7d629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to create TFIDF:\n",
    "# Bring into your notebook the functions you created previously to generate a BOW table and compute TFIDF values. \n",
    "# Extend the TFIDF function so that it also returns the DFIDF value for each term in the VOCAB.\n",
    "def gather_docs(CORPUS, ohco_level, term_col='term_str'):\n",
    "    OHCO = CORPUS.index.names\n",
    "    CORPUS[term_col] = CORPUS[term_col].astype('str')\n",
    "    DOC = CORPUS.groupby(OHCO[:ohco_level])[term_col].apply(lambda x:' '.join(x)).to_frame('doc_str')\n",
    "    return DOC\n",
    "\n",
    "def BOW(tokendf, ocholevel):\n",
    "    return tokendf.groupby(bags[ocholevel]+['term_str']).term_str.count().to_frame('n') \n",
    "\n",
    "def BOWtoTFIDF(BOW, tf_method, CORPUS, ocholevel = 2):\n",
    "    \n",
    "    # I added another parameter for the CORPUS because otherwise it becomes really tedious to undo the BOW function\n",
    "\n",
    "    DTCM = BOW.n.unstack(fill_value=0)\n",
    "    if tf_method == 'sum':\n",
    "        TF = DTCM.T / DTCM.T.sum()\n",
    "    elif tf_method == 'max':\n",
    "        TF = DTCM.T / DTCM.T.max()\n",
    "    elif tf_method == 'log':\n",
    "        TF = np.log2(1 + DTCM.T)\n",
    "    elif tf_method == 'raw':\n",
    "        TF = DTCM.T\n",
    "    elif tf_method == 'double_norm':\n",
    "        TF = DTCM.T / DTCM.T.max()\n",
    "    elif tf_method == 'binary':\n",
    "        TF = DTCM.T.astype('bool').astype('int')\n",
    "    TF = TF.T\n",
    "    DF = DTCM.astype('bool').sum() \n",
    "    N = DTCM.shape[0]\n",
    "    IDF = IDF = np.log2(N / DF)\n",
    "    TFIDF = TF * IDF\n",
    "    # Extend the TFIDF function so that it also returns the DFIDF value for each term in the VOCAB.\n",
    "\n",
    "    # We can adjust ocho level here if we want to, default is 2\n",
    "    DOC = gather_docs(CORPUS, 2)\n",
    "    DOC['n_tokens'] = DOC.doc_str.apply(lambda x: len(x.split()))\n",
    "    ngram_range = (1,2)\n",
    "    n_terms = 4000\n",
    "    count_engine = CountVectorizer(\n",
    "        stop_words = 'english',\n",
    "        ngram_range = ngram_range,\n",
    "        max_features = n_terms)\n",
    "    X = count_engine.fit_transform(DOC.doc_str)\n",
    "    DTM = pd.DataFrame(X.toarray(), \n",
    "    columns=count_engine.get_feature_names_out(), \n",
    "    index=DOC.index)\n",
    "    VOCAB = DTM.sum().to_frame('n')\n",
    "    VOCAB.index.name = 'term_str'\n",
    "    VOCAB['df'] = DTM.astype(bool).sum()\n",
    "    VOCAB['dfidf'] = VOCAB.df * np.log2(len(DTM)/VOCAB.df)\n",
    "    VOCAB['dp'] = VOCAB.df / len(DTM)\n",
    "    VOCAB['di'] = np.log2(1/VOCAB.dp)\n",
    "    VOCAB['dh'] = VOCAB.dp * VOCAB.di\n",
    "    VOCAB['n_chars'] = VOCAB.apply(lambda x: len(x.name), 1)\n",
    "    VOCAB['n_tokens'] = VOCAB.apply(lambda x: len(x.name.split()), 1)\n",
    "    VOCAB.sort_index()\n",
    "\n",
    "\n",
    "    #Hopefully this join works but I'm actually not too sure this will work\n",
    "    \n",
    "    return TFIDF,VOCAB,DTM\n",
    "\n",
    "# I think I'm going to use this oneliner more.\n",
    "# get_tfidf = lambda X, agg_func='sum': (X.T / X.T.agg(agg_func)).T * (np.log2(len(X)/X.astype('bool').sum()))\n",
    "OHCO = ['book_id','chap_id', 'para_num', 'sent_num', 'token_num']\n",
    "\n",
    "bags = dict(\n",
    "    SENTS = OHCO[:4],\n",
    "    PARAS = OHCO[:3],\n",
    "    CHAPS = OHCO[:2],\n",
    "    BOOKS = OHCO[:1]\n",
    ")\n",
    "tables = BOWtoTFIDF(BOW(CORPUS,'CHAPS'),'max', CORPUS, 2)\n",
    "DTM = tables[2]\n",
    "DFIDF = tables[1]\n",
    "TFIDF = tables[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf1f1ab0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "term_str\n",
       "abandoned     34.710943\n",
       "abandoning    21.265923\n",
       "abilities     23.253497\n",
       "ability       28.073549\n",
       "able          18.027973\n",
       "                ...    \n",
       "ﬁrst time     25.034264\n",
       "ﬁst           23.253497\n",
       "ﬁt            25.034264\n",
       "ﬁxed          21.265923\n",
       "ﬂoor          30.531846\n",
       "Name: dfidf, Length: 4000, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DFIDF['dfidf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "449bcb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB = VOCAB.join(DFIDF['dfidf'])\n",
    "VOCAB['dfidf'] = VOCAB['dfidf'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd5bfa5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>n_chars</th>\n",
       "      <th>p</th>\n",
       "      <th>i</th>\n",
       "      <th>max_pos</th>\n",
       "      <th>max_pos_group</th>\n",
       "      <th>n_pos_group</th>\n",
       "      <th>cat_pos_group</th>\n",
       "      <th>n_pos</th>\n",
       "      <th>cat_pos</th>\n",
       "      <th>stop</th>\n",
       "      <th>porter_stem</th>\n",
       "      <th>stem_snowball</th>\n",
       "      <th>stem_lancaster</th>\n",
       "      <th>dfidf</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>term_str</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>reasons</th>\n",
       "      <td>50</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>13.046759</td>\n",
       "      <td>NNS</td>\n",
       "      <td>NN</td>\n",
       "      <td>2</td>\n",
       "      <td>{VB, NN}</td>\n",
       "      <td>3</td>\n",
       "      <td>{VB, NNS, NN}</td>\n",
       "      <td>0</td>\n",
       "      <td>reason</td>\n",
       "      <td>reason</td>\n",
       "      <td>reason</td>\n",
       "      <td>37.149926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rooms</th>\n",
       "      <td>81</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>12.350765</td>\n",
       "      <td>NNS</td>\n",
       "      <td>NN</td>\n",
       "      <td>3</td>\n",
       "      <td>{VB, JJ, NN}</td>\n",
       "      <td>4</td>\n",
       "      <td>{VBZ, NNS, JJ, NN}</td>\n",
       "      <td>0</td>\n",
       "      <td>room</td>\n",
       "      <td>room</td>\n",
       "      <td>room</td>\n",
       "      <td>37.149926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>white</th>\n",
       "      <td>54</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>12.935728</td>\n",
       "      <td>JJ</td>\n",
       "      <td>JJ</td>\n",
       "      <td>2</td>\n",
       "      <td>{JJ, NN}</td>\n",
       "      <td>2</td>\n",
       "      <td>{JJ, NN}</td>\n",
       "      <td>0</td>\n",
       "      <td>white</td>\n",
       "      <td>white</td>\n",
       "      <td>whit</td>\n",
       "      <td>37.149926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fetch</th>\n",
       "      <td>52</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>12.990176</td>\n",
       "      <td>VB</td>\n",
       "      <td>VB</td>\n",
       "      <td>2</td>\n",
       "      <td>{VB, NN}</td>\n",
       "      <td>3</td>\n",
       "      <td>{VB, VBP, NN}</td>\n",
       "      <td>0</td>\n",
       "      <td>fetch</td>\n",
       "      <td>fetch</td>\n",
       "      <td>fetch</td>\n",
       "      <td>37.149926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>appearance</th>\n",
       "      <td>42</td>\n",
       "      <td>10</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>13.298298</td>\n",
       "      <td>NN</td>\n",
       "      <td>NN</td>\n",
       "      <td>2</td>\n",
       "      <td>{JJ, NN}</td>\n",
       "      <td>2</td>\n",
       "      <td>{JJ, NN}</td>\n",
       "      <td>0</td>\n",
       "      <td>appear</td>\n",
       "      <td>appear</td>\n",
       "      <td>appear</td>\n",
       "      <td>37.149926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>changed</th>\n",
       "      <td>50</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>13.046759</td>\n",
       "      <td>VBN</td>\n",
       "      <td>VB</td>\n",
       "      <td>3</td>\n",
       "      <td>{VB, JJ, NN}</td>\n",
       "      <td>7</td>\n",
       "      <td>{VBZ, NN, JJ, VBN, VBP, NNS, VBD}</td>\n",
       "      <td>0</td>\n",
       "      <td>chang</td>\n",
       "      <td>chang</td>\n",
       "      <td>chang</td>\n",
       "      <td>37.149926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>calmly</th>\n",
       "      <td>41</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>13.333063</td>\n",
       "      <td>RB</td>\n",
       "      <td>NN</td>\n",
       "      <td>4</td>\n",
       "      <td>{VB, JJ, RB, NN}</td>\n",
       "      <td>8</td>\n",
       "      <td>{VB, VBZ, NN, JJ, VBP, NNS, VBD, RB}</td>\n",
       "      <td>0</td>\n",
       "      <td>calmli</td>\n",
       "      <td>calm</td>\n",
       "      <td>calm</td>\n",
       "      <td>37.149926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thrown</th>\n",
       "      <td>45</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>13.198762</td>\n",
       "      <td>VBN</td>\n",
       "      <td>VB</td>\n",
       "      <td>5</td>\n",
       "      <td>{VB, NN, IN, JJ, RP}</td>\n",
       "      <td>7</td>\n",
       "      <td>{VB, NN, IN, JJ, VBN, VBP, RP}</td>\n",
       "      <td>0</td>\n",
       "      <td>thrown</td>\n",
       "      <td>thrown</td>\n",
       "      <td>thrown</td>\n",
       "      <td>37.149926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walk</th>\n",
       "      <td>66</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000156</td>\n",
       "      <td>12.646221</td>\n",
       "      <td>VB</td>\n",
       "      <td>VB</td>\n",
       "      <td>2</td>\n",
       "      <td>{VB, NN}</td>\n",
       "      <td>3</td>\n",
       "      <td>{VB, VBP, NN}</td>\n",
       "      <td>0</td>\n",
       "      <td>walk</td>\n",
       "      <td>walk</td>\n",
       "      <td>walk</td>\n",
       "      <td>37.149926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>heart</th>\n",
       "      <td>45</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>13.198762</td>\n",
       "      <td>NN</td>\n",
       "      <td>NN</td>\n",
       "      <td>1</td>\n",
       "      <td>{NN}</td>\n",
       "      <td>2</td>\n",
       "      <td>{NNS, NN}</td>\n",
       "      <td>0</td>\n",
       "      <td>heart</td>\n",
       "      <td>heart</td>\n",
       "      <td>heart</td>\n",
       "      <td>37.149926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>directly</th>\n",
       "      <td>41</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>13.333063</td>\n",
       "      <td>RB</td>\n",
       "      <td>RB</td>\n",
       "      <td>3</td>\n",
       "      <td>{VB, RB, NN}</td>\n",
       "      <td>4</td>\n",
       "      <td>{VB, RB, VBP, NN}</td>\n",
       "      <td>0</td>\n",
       "      <td>directli</td>\n",
       "      <td>direct</td>\n",
       "      <td>direct</td>\n",
       "      <td>37.149926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>caused</th>\n",
       "      <td>44</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>13.231184</td>\n",
       "      <td>VBN</td>\n",
       "      <td>VB</td>\n",
       "      <td>1</td>\n",
       "      <td>{VB}</td>\n",
       "      <td>2</td>\n",
       "      <td>{VBN, VBD}</td>\n",
       "      <td>0</td>\n",
       "      <td>caus</td>\n",
       "      <td>caus</td>\n",
       "      <td>caus</td>\n",
       "      <td>37.149926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>naturally</th>\n",
       "      <td>51</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>13.018190</td>\n",
       "      <td>RB</td>\n",
       "      <td>RB</td>\n",
       "      <td>4</td>\n",
       "      <td>{IN, RB, JJ, NN}</td>\n",
       "      <td>6</td>\n",
       "      <td>{NN, IN, JJ, NNS, RB, NNP}</td>\n",
       "      <td>0</td>\n",
       "      <td>natur</td>\n",
       "      <td>natur</td>\n",
       "      <td>nat</td>\n",
       "      <td>37.149926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>castle</th>\n",
       "      <td>325</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000768</td>\n",
       "      <td>10.346319</td>\n",
       "      <td>NN</td>\n",
       "      <td>NN</td>\n",
       "      <td>3</td>\n",
       "      <td>{VB, JJ, NN}</td>\n",
       "      <td>4</td>\n",
       "      <td>{JJ, VBP, NN, NNP}</td>\n",
       "      <td>0</td>\n",
       "      <td>castl</td>\n",
       "      <td>castl</td>\n",
       "      <td>castl</td>\n",
       "      <td>37.149926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spoken</th>\n",
       "      <td>45</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>13.198762</td>\n",
       "      <td>VBN</td>\n",
       "      <td>VB</td>\n",
       "      <td>2</td>\n",
       "      <td>{VB, NN}</td>\n",
       "      <td>2</td>\n",
       "      <td>{VBN, NN}</td>\n",
       "      <td>0</td>\n",
       "      <td>spoken</td>\n",
       "      <td>spoken</td>\n",
       "      <td>spok</td>\n",
       "      <td>37.149926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pleased</th>\n",
       "      <td>40</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>13.368687</td>\n",
       "      <td>JJ</td>\n",
       "      <td>JJ</td>\n",
       "      <td>3</td>\n",
       "      <td>{VB, JJ, NN}</td>\n",
       "      <td>6</td>\n",
       "      <td>{VBZ, NN, JJ, VBN, VBP, VBD}</td>\n",
       "      <td>0</td>\n",
       "      <td>pleas</td>\n",
       "      <td>pleas</td>\n",
       "      <td>pleas</td>\n",
       "      <td>37.149926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unfortunately</th>\n",
       "      <td>62</td>\n",
       "      <td>13</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>12.736419</td>\n",
       "      <td>RB</td>\n",
       "      <td>RB</td>\n",
       "      <td>5</td>\n",
       "      <td>{VB, NN, IN, JJ, RB}</td>\n",
       "      <td>6</td>\n",
       "      <td>{NN, IN, JJ, VBP, RB, NNP}</td>\n",
       "      <td>0</td>\n",
       "      <td>unfortun</td>\n",
       "      <td>unfortun</td>\n",
       "      <td>unfortun</td>\n",
       "      <td>37.149926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>visit</th>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>12.690615</td>\n",
       "      <td>NN</td>\n",
       "      <td>NN</td>\n",
       "      <td>2</td>\n",
       "      <td>{VB, NN}</td>\n",
       "      <td>3</td>\n",
       "      <td>{VB, VBP, NN}</td>\n",
       "      <td>0</td>\n",
       "      <td>visit</td>\n",
       "      <td>visit</td>\n",
       "      <td>visit</td>\n",
       "      <td>37.149926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fall</th>\n",
       "      <td>58</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000137</td>\n",
       "      <td>12.832634</td>\n",
       "      <td>VB</td>\n",
       "      <td>VB</td>\n",
       "      <td>3</td>\n",
       "      <td>{VB, JJ, NN}</td>\n",
       "      <td>4</td>\n",
       "      <td>{VB, JJ, VBP, NN}</td>\n",
       "      <td>0</td>\n",
       "      <td>fall</td>\n",
       "      <td>fall</td>\n",
       "      <td>fal</td>\n",
       "      <td>37.149926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>supposed</th>\n",
       "      <td>50</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>13.046759</td>\n",
       "      <td>VBN</td>\n",
       "      <td>VB</td>\n",
       "      <td>2</td>\n",
       "      <td>{VB, JJ}</td>\n",
       "      <td>3</td>\n",
       "      <td>{JJ, VBN, VBD}</td>\n",
       "      <td>0</td>\n",
       "      <td>suppos</td>\n",
       "      <td>suppos</td>\n",
       "      <td>suppos</td>\n",
       "      <td>37.149926</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 n  n_chars         p          i max_pos max_pos_group  \\\n",
       "term_str                                                                 \n",
       "reasons         50        7  0.000118  13.046759     NNS            NN   \n",
       "rooms           81        5  0.000191  12.350765     NNS            NN   \n",
       "white           54        5  0.000128  12.935728      JJ            JJ   \n",
       "fetch           52        5  0.000123  12.990176      VB            VB   \n",
       "appearance      42       10  0.000099  13.298298      NN            NN   \n",
       "changed         50        7  0.000118  13.046759     VBN            VB   \n",
       "calmly          41        6  0.000097  13.333063      RB            NN   \n",
       "thrown          45        6  0.000106  13.198762     VBN            VB   \n",
       "walk            66        4  0.000156  12.646221      VB            VB   \n",
       "heart           45        5  0.000106  13.198762      NN            NN   \n",
       "directly        41        8  0.000097  13.333063      RB            RB   \n",
       "caused          44        6  0.000104  13.231184     VBN            VB   \n",
       "naturally       51        9  0.000121  13.018190      RB            RB   \n",
       "castle         325        6  0.000768  10.346319      NN            NN   \n",
       "spoken          45        6  0.000106  13.198762     VBN            VB   \n",
       "pleased         40        7  0.000095  13.368687      JJ            JJ   \n",
       "unfortunately   62       13  0.000147  12.736419      RB            RB   \n",
       "visit           64        5  0.000151  12.690615      NN            NN   \n",
       "fall            58        4  0.000137  12.832634      VB            VB   \n",
       "supposed        50        8  0.000118  13.046759     VBN            VB   \n",
       "\n",
       "               n_pos_group         cat_pos_group  n_pos  \\\n",
       "term_str                                                  \n",
       "reasons                  2              {VB, NN}      3   \n",
       "rooms                    3          {VB, JJ, NN}      4   \n",
       "white                    2              {JJ, NN}      2   \n",
       "fetch                    2              {VB, NN}      3   \n",
       "appearance               2              {JJ, NN}      2   \n",
       "changed                  3          {VB, JJ, NN}      7   \n",
       "calmly                   4      {VB, JJ, RB, NN}      8   \n",
       "thrown                   5  {VB, NN, IN, JJ, RP}      7   \n",
       "walk                     2              {VB, NN}      3   \n",
       "heart                    1                  {NN}      2   \n",
       "directly                 3          {VB, RB, NN}      4   \n",
       "caused                   1                  {VB}      2   \n",
       "naturally                4      {IN, RB, JJ, NN}      6   \n",
       "castle                   3          {VB, JJ, NN}      4   \n",
       "spoken                   2              {VB, NN}      2   \n",
       "pleased                  3          {VB, JJ, NN}      6   \n",
       "unfortunately            5  {VB, NN, IN, JJ, RB}      6   \n",
       "visit                    2              {VB, NN}      3   \n",
       "fall                     3          {VB, JJ, NN}      4   \n",
       "supposed                 2              {VB, JJ}      3   \n",
       "\n",
       "                                            cat_pos  stop porter_stem  \\\n",
       "term_str                                                                \n",
       "reasons                               {VB, NNS, NN}     0      reason   \n",
       "rooms                            {VBZ, NNS, JJ, NN}     0        room   \n",
       "white                                      {JJ, NN}     0       white   \n",
       "fetch                                 {VB, VBP, NN}     0       fetch   \n",
       "appearance                                 {JJ, NN}     0      appear   \n",
       "changed           {VBZ, NN, JJ, VBN, VBP, NNS, VBD}     0       chang   \n",
       "calmly         {VB, VBZ, NN, JJ, VBP, NNS, VBD, RB}     0      calmli   \n",
       "thrown               {VB, NN, IN, JJ, VBN, VBP, RP}     0      thrown   \n",
       "walk                                  {VB, VBP, NN}     0        walk   \n",
       "heart                                     {NNS, NN}     0       heart   \n",
       "directly                          {VB, RB, VBP, NN}     0    directli   \n",
       "caused                                   {VBN, VBD}     0        caus   \n",
       "naturally                {NN, IN, JJ, NNS, RB, NNP}     0       natur   \n",
       "castle                           {JJ, VBP, NN, NNP}     0       castl   \n",
       "spoken                                    {VBN, NN}     0      spoken   \n",
       "pleased                {VBZ, NN, JJ, VBN, VBP, VBD}     0       pleas   \n",
       "unfortunately            {NN, IN, JJ, VBP, RB, NNP}     0    unfortun   \n",
       "visit                                 {VB, VBP, NN}     0       visit   \n",
       "fall                              {VB, JJ, VBP, NN}     0        fall   \n",
       "supposed                             {JJ, VBN, VBD}     0      suppos   \n",
       "\n",
       "              stem_snowball stem_lancaster      dfidf  \n",
       "term_str                                               \n",
       "reasons              reason         reason  37.149926  \n",
       "rooms                  room           room  37.149926  \n",
       "white                 white           whit  37.149926  \n",
       "fetch                 fetch          fetch  37.149926  \n",
       "appearance           appear         appear  37.149926  \n",
       "changed               chang          chang  37.149926  \n",
       "calmly                 calm           calm  37.149926  \n",
       "thrown               thrown         thrown  37.149926  \n",
       "walk                   walk           walk  37.149926  \n",
       "heart                 heart          heart  37.149926  \n",
       "directly             direct         direct  37.149926  \n",
       "caused                 caus           caus  37.149926  \n",
       "naturally             natur            nat  37.149926  \n",
       "castle                castl          castl  37.149926  \n",
       "spoken               spoken           spok  37.149926  \n",
       "pleased               pleas          pleas  37.149926  \n",
       "unfortunately      unfortun       unfortun  37.149926  \n",
       "visit                 visit          visit  37.149926  \n",
       "fall                   fall            fal  37.149926  \n",
       "supposed             suppos         suppos  37.149926  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB.sort_values('dfidf',ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b7f20d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#VOCAB.to_csv('VOCAB.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
