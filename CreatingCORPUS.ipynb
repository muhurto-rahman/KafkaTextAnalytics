{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6174bdd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing 5200 METAMORPHOSIS\n",
      "Importing  /Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/KafkaFinal/data\\pg5200.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone ^\\s*[IVXLCM]+\\s*$\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n",
      "Tokenizing 5252 BLUMFELD\n",
      "Importing  /Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/KafkaFinal/data\\pg5252.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone NOCHAPTERS\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n",
      "Tokenizing 5353 THE BURROW\n",
      "Importing  /Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/KafkaFinal/data\\pg5353.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone NOCHAPTERS\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n",
      "Tokenizing 5454 JOSEPHINE THE SONGSTRESS\n",
      "Importing  /Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/KafkaFinal/data\\pg5454.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone NOCHAPTERS\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n",
      "Tokenizing 5555 BEFORE THE LAW\n",
      "Importing  /Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/KafkaFinal/data\\pg5555.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone NOCHAPTERS\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n",
      "Tokenizing 5656 UP IN THE GALLERY\n",
      "Importing  /Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/KafkaFinal/data\\pg5656.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone NOCHAPTERS\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n",
      "Tokenizing 5757 THE HUNTER GRACCHUS\n",
      "Importing  /Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/KafkaFinal/data\\pg5757.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone NOCHAPTERS\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n",
      "Tokenizing 5858 THE GREAT WALL OF CHINA\n",
      "Importing  /Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/KafkaFinal/data\\pg5858.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone NOCHAPTERS\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n",
      "Tokenizing 5959 A REPORT FOR AN ACADEMY\n",
      "Importing  /Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/KafkaFinal/data\\pg5959.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone NOCHAPTERS\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n",
      "Tokenizing 6060 DEAREST FATHER\n",
      "Importing  /Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/KafkaFinal/data\\pg6060.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone NOCHAPTERS\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n",
      "Tokenizing 6161 THE JUDGEMENT\n",
      "Importing  /Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/KafkaFinal/data\\pg6161.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone NOCHAPTERS\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n",
      "Tokenizing 6262 AMERIKA\n",
      "Importing  /Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/KafkaFinal/data\\pg6262.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone ^\\s*\\d+\\s*$\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n",
      "Tokenizing 6363 IN THE PENAL COLONY\n",
      "Importing  /Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/KafkaFinal/data\\pg6363.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone NOCHAPTERS\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n",
      "Tokenizing 6464 THE HUNGER ARTIST\n",
      "Importing  /Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/KafkaFinal/data\\pg6464.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone NOCHAPTERS\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n",
      "Tokenizing 6565 THE JACKALS AND ARABS\n",
      "Importing  /Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/KafkaFinal/data\\pg6565.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone NOCHAPTERS\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n",
      "Tokenizing 6666 A COUNTRY DOCTOR\n",
      "Importing  /Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/KafkaFinal/data\\pg6666.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone NOCHAPTERS\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n",
      "Tokenizing 6767 AN IMPERIAL MESSAGE\n",
      "Importing  /Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/KafkaFinal/data\\pg6767.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone NOCHAPTERS\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n",
      "Tokenizing 6969 THE CASTLE\n",
      "Importing  /Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/KafkaFinal/data\\pg6969.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone ^\\s*\\d+\\s*$\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n",
      "Tokenizing 7849 THE TRIAL\n",
      "Importing  /Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/KafkaFinal/data\\pg7849.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone ^\\s*Chapter\\s+(?:One|Two|Three|Four|Five|Six|Seven|Eight|Nine|Ten)\\s*$\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n",
      "Tokenizing 23532 MEDITATION\n",
      "Importing  /Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/KafkaFinal/data\\pg23532.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone ^(Children on the country road|Unmasking a con artist|The Sudden Walk|Resolutions|The trip to the mountains|The Bachelor's Misfortune|The Merchant|Distracted Looking Out|The Way Home|The Passers-by|Passenger|Dresses|The rejection|Food for thought for gentlemen riders|The Alley Window|Desire to become an Indian|The Trees|Unhappiness)$\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/DS5001-2025-01-R/lessons/lib\\textparser.py:132: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  div_lines = self.TOKENS[src_col].str.contains(div_pat, regex=True, case=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import re\n",
    "import nltk\n",
    "import plotly_express as px\n",
    "import configparser\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import re\n",
    "import nltk\n",
    "import plotly_express as px\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "import scipy.cluster.hierarchy as sch\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.linalg import norm\n",
    "from scipy.spatial.distance import pdist\n",
    "import plotly_express as px\n",
    "import seaborn as sns; sns.set()\n",
    "import configparser\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\"../../../env-sample.ini\")\n",
    "data_home = '/Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/KafkaFinal/data'\n",
    "output_dir = '/Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/KafkaFinal/output'\n",
    "local_lib = '/Users/muhur/OneDrive/Desktop/Muhurto/Data Science Grad School/DS5001/DS5001-2025-01-R/lessons/lib'\n",
    "import sys\n",
    "sys.path.append(local_lib)\n",
    "from textparser import TextParser\n",
    "\n",
    "clip_pats = [\n",
    "    r\"\\*\\*\\*\\s*START OF\",\n",
    "    r\"\\*\\*\\*\\s*END OF\"\n",
    "]\n",
    "\n",
    "# All are 'chap'and 'm'\n",
    "roman = '[IVXLCM]+'\n",
    "caps = \"[A-Z';, -]+\"\n",
    "ohco_pat_list = [\n",
    "    (5200,   rf\"^\\s*CHAPTER\\s+{roman}\\s*$\"), #Metamorphosis\n",
    "    (7849,   rf\"^\\s*{roman}\\s*$\"), #The Trial\n",
    "    (6969,  rf\"^\\s*LETTER .* to .*$\"), # The Castle\n",
    "    (6262,   rf\"^CHAPTER\\s+{roman}$\"), # Amerika\n",
    "    (6161,   rf\"^CHAPTER\\s+\\d+$\"), # The Judgement\n",
    "    (6060,   rf\"^Chapter\\s+\\d+$\"), # Dearest Father\n",
    "    (6363,  rf\"^Chapter\\s+\\d+$\"), # In the Penal colony\n",
    "    (6464,   rf\"^CHAPTER\\s+\\d+$\"), # The Hunger Artist\n",
    "    (6565, rf\"^\\s*CHAPTER\\s+{roman}\\.\"), # The Jackals and Arabs\n",
    "    (6666, rf\"^\\s*CHAPTER\\s+{roman}\\s*$\"), # A Country Doctor\n",
    "    (6767, rf\"^\\s*CHAPTER\\s+{roman}\\s*$\"), # An Imperial Message\n",
    "    (5959,  rf\"^(?:ETYMOLOGY|EXTRACTS|CHAPTER)\"), # A report for an Academy\n",
    "    (5858,  rf\"^\\s*CHAPTER\\s+{roman}\\.\\s*$\"), # The Great Wall of China\n",
    "    (5757, rf\"^\\s*{roman}\\.\\s*$\"), # The Hunter Gracchus\n",
    "    (5656,  rf\"^\\s*{roman}\\. .*$\"), # Up in the Gallery\n",
    "    (5555, rf\"^CHAPTER\\s+{roman}\\.?$\"), # Before the Law\n",
    "    (5454, rf\"^\\s*[A-Z,;-]+\\.\\s*$\"), # Josephine the Songstress\n",
    "    (5353,  rf\"^CHAPTER \"), # The Burrow\n",
    "    (5252, rf\"^CHAPTER\\s+{roman}\\.\\s*$\"), # Blumfeld\n",
    "    (23532, rf\"Chapter\\s+{roman}\") # Meditation\n",
    "]\n",
    "chapter_regexes = [\n",
    "    (5200,   rf\"^\\s*{roman}\\s*$\"),\n",
    "    (7849,   rf\"^\\s*Chapter\\s+(?:One|Two|Three|Four|Five|Six|Seven|Eight|Nine|Ten)\\s*$\"),\n",
    "    (6969,   rf\"^\\s*\\d+\\s*$\"),\n",
    "    (6262,   rf\"^\\s*\\d+\\s*$\"),\n",
    "    (6161,   \"NOCHAPTERS\"),\n",
    "    (6060,   \"NOCHAPTERS\"),\n",
    "    (6363,   \"NOCHAPTERS\"),\n",
    "    (6464,   \"NOCHAPTERS\"),\n",
    "    (6565,   \"NOCHAPTERS\"),\n",
    "    (6666,   \"NOCHAPTERS\"),\n",
    "    (6767,   \"NOCHAPTERS\"),\n",
    "    (5959,   \"NOCHAPTERS\"),\n",
    "    (5858,   \"NOCHAPTERS\"),\n",
    "    (5757,   \"NOCHAPTERS\"),\n",
    "    (5656,   \"NOCHAPTERS\"),\n",
    "    (5555,   \"NOCHAPTERS\"),\n",
    "    (5454,   \"NOCHAPTERS\"),\n",
    "    (5353,   \"NOCHAPTERS\"),\n",
    "    (5252,   \"NOCHAPTERS\"),\n",
    "    (23532,  rf\"^(Children on the country road|Unmasking a con artist|The Sudden Walk|Resolutions|The trip to the mountains|The Bachelor's Misfortune|The Merchant|Distracted Looking Out|The Way Home|The Passers-by|Passenger|Dresses|The rejection|Food for thought for gentlemen riders|The Alley Window|Desire to become an Indian|The Trees|Unhappiness)$\")  # Poem title on line 1\n",
    "]\n",
    "ohco_pat_list = chapter_regexes\n",
    "source_files = f'{data_home}'\n",
    "source_file_list = sorted(glob(f\"{source_files}/*.*\"))\n",
    "\n",
    "book_data = []\n",
    "for source_file_path in source_file_list:\n",
    "    # Get the filename only, e.g. 'pg5353.txt'\n",
    "    filename = os.path.basename(source_file_path)\n",
    "    # Extract the numeric ID from the filename (remove 'pg' and '.txt')\n",
    "    book_id = int(filename.replace('pg', '').replace('.txt', ''))\n",
    "    # Use filename (without extension) as a raw title (optional: clean further)\n",
    "    book_title = filename.replace('.txt', '').replace('_', ' ')\n",
    "    # Append a tuple of (book_id, path, title)\n",
    "    book_data.append((book_id, source_file_path, book_title))\n",
    "# Convert to DataFrame\n",
    "LIB = pd.DataFrame(book_data, columns=['book_id', 'source_file_path', 'raw_title']) \\\n",
    "        .set_index('book_id') \\\n",
    "        .sort_index()\n",
    "book_titles = {\n",
    "    5200: \"Metamorphosis\",\n",
    "    7849: \"The Trial\",\n",
    "    6969: \"The Castle\",\n",
    "    6262: \"Amerika\",\n",
    "    6161: \"The Judgement\",\n",
    "    6060: \"Dearest Father\",\n",
    "    6363: \"In the Penal Colony\",\n",
    "    6464: \"The Hunger Artist\",\n",
    "    6565: \"The Jackals and Arabs\",\n",
    "    6666: \"A Country Doctor\",\n",
    "    6767: \"An Imperial Message\",\n",
    "    5959: \"A Report for an Academy\",\n",
    "    5858: \"The Great Wall of China\",\n",
    "    5757: \"The Hunter Gracchus\",\n",
    "    5656: \"Up in the Gallery\",\n",
    "    5555: \"Before the Law\",\n",
    "    5454: \"Josephine the Songstress\",\n",
    "    5353: \"The Burrow\",\n",
    "    5252: \"Blumfeld\",\n",
    "    23532: \"Meditation\"\n",
    "}\n",
    "book_titles = {f'pg{key}': value for key, value in book_titles.items()}\n",
    "try:\n",
    "    LIB['author'] = 'KAFKA, FRANZ'\n",
    "    LIB['title'] = LIB.raw_title.replace(book_titles).str.upper()\n",
    "    LIB = LIB.drop('raw_title', axis=1)\n",
    "except AttributeError:\n",
    "    pass\n",
    "LIB['chap_regex'] = LIB.index.map(pd.Series({x[0]:x[1] for x in ohco_pat_list}))\n",
    "LIB\n",
    "# This cell takes 16 seconds to run\n",
    "def tokenize_collection(LIB):\n",
    "\n",
    "    clip_pats = [\n",
    "        r\"\\*\\*\\*\\s*START OF\",\n",
    "        r\"\\*\\*\\*\\s*END OF\"\n",
    "    ]\n",
    "\n",
    "    books = []\n",
    "    for book_id in LIB.index:\n",
    "\n",
    "        # Announce\n",
    "        print(\"Tokenizing\", book_id, LIB.loc[book_id].title)\n",
    "\n",
    "        # Define vars\n",
    "        chap_regex = LIB.loc[book_id].chap_regex\n",
    "        ohco_pats = [('chap', chap_regex, 'm')]\n",
    "        src_file_path = LIB.loc[book_id].source_file_path\n",
    "\n",
    "        # Create object\n",
    "        text = TextParser(src_file_path, ohco_pats=ohco_pats, clip_pats=clip_pats, use_nltk=True)\n",
    "        # text = TextImporter(src_file_path, ohco_pats=ohco_pats, clip_pats=clip_pats) \n",
    "\n",
    "        # Define parameters\n",
    "        text.verbose = True\n",
    "        text.strip_hyphens = True\n",
    "        text.strip_whitespace = True\n",
    "\n",
    "        # Parse\n",
    "        text.import_source().parse_tokens();\n",
    "\n",
    "        # Name things\n",
    "        text.TOKENS['book_id'] = book_id\n",
    "        text.TOKENS = text.TOKENS.reset_index().set_index(['book_id'] + text.OHCO)\n",
    "\n",
    "        # Add to list\n",
    "        books.append(text.TOKENS)\n",
    "        \n",
    "    # Combine into a single dataframe\n",
    "    CORPUS = pd.concat(books).sort_index()\n",
    "\n",
    "    # Clean up\n",
    "    del(books)\n",
    "    del(text)\n",
    "        \n",
    "    print(\"Done\")\n",
    "        \n",
    "    return CORPUS\n",
    "CORPUS = tokenize_collection(LIB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "995b7711",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>pos_tuple</th>\n",
       "      <th>pos</th>\n",
       "      <th>token_str</th>\n",
       "      <th>term_str</th>\n",
       "      <th>pos_group</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>book_id</th>\n",
       "      <th>chap_id</th>\n",
       "      <th>para_num</th>\n",
       "      <th>sent_num</th>\n",
       "      <th>token_num</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">5200</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th>0</th>\n",
       "      <td>(One, CD)</td>\n",
       "      <td>CD</td>\n",
       "      <td>One</td>\n",
       "      <td>one</td>\n",
       "      <td>CD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(morning,, NN)</td>\n",
       "      <td>NN</td>\n",
       "      <td>morning,</td>\n",
       "      <td>morning</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(when, WRB)</td>\n",
       "      <td>WRB</td>\n",
       "      <td>when</td>\n",
       "      <td>when</td>\n",
       "      <td>WR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Gregor, NNP)</td>\n",
       "      <td>NNP</td>\n",
       "      <td>Gregor</td>\n",
       "      <td>gregor</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(Samsa, NNP)</td>\n",
       "      <td>NNP</td>\n",
       "      <td>Samsa</td>\n",
       "      <td>samsa</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">23532</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">10</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">39</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th>27</th>\n",
       "      <td>(upstairs, JJ)</td>\n",
       "      <td>JJ</td>\n",
       "      <td>upstairs</td>\n",
       "      <td>upstairs</td>\n",
       "      <td>JJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>(and, CC)</td>\n",
       "      <td>CC</td>\n",
       "      <td>and</td>\n",
       "      <td>and</td>\n",
       "      <td>CC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>(go, VB)</td>\n",
       "      <td>VB</td>\n",
       "      <td>go</td>\n",
       "      <td>go</td>\n",
       "      <td>VB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>(to, TO)</td>\n",
       "      <td>TO</td>\n",
       "      <td>to</td>\n",
       "      <td>to</td>\n",
       "      <td>TO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>(sleep., VB)</td>\n",
       "      <td>VB</td>\n",
       "      <td>sleep.</td>\n",
       "      <td>sleep</td>\n",
       "      <td>VB</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>423144 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  pos_tuple  pos token_str  \\\n",
       "book_id chap_id para_num sent_num token_num                                  \n",
       "5200    1       1        0        0               (One, CD)   CD       One   \n",
       "                                  1          (morning,, NN)   NN  morning,   \n",
       "                                  2             (when, WRB)  WRB      when   \n",
       "                                  3           (Gregor, NNP)  NNP    Gregor   \n",
       "                                  4            (Samsa, NNP)  NNP     Samsa   \n",
       "...                                                     ...  ...       ...   \n",
       "23532   10      39       0        27         (upstairs, JJ)   JJ  upstairs   \n",
       "                                  28              (and, CC)   CC       and   \n",
       "                                  29               (go, VB)   VB        go   \n",
       "                                  30               (to, TO)   TO        to   \n",
       "                                  31           (sleep., VB)   VB    sleep.   \n",
       "\n",
       "                                             term_str pos_group  \n",
       "book_id chap_id para_num sent_num token_num                      \n",
       "5200    1       1        0        0               one        CD  \n",
       "                                  1           morning        NN  \n",
       "                                  2              when        WR  \n",
       "                                  3            gregor        NN  \n",
       "                                  4             samsa        NN  \n",
       "...                                               ...       ...  \n",
       "23532   10      39       0        27         upstairs        JJ  \n",
       "                                  28              and        CC  \n",
       "                                  29               go        VB  \n",
       "                                  30               to        TO  \n",
       "                                  31            sleep        VB  \n",
       "\n",
       "[423144 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CORPUS = CORPUS[CORPUS.term_str != '']\n",
    "CORPUS['pos_group'] = CORPUS.pos.str[:2]\n",
    "CORPUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ac3a057",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CORPUS.to_csv('CORPUS.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
